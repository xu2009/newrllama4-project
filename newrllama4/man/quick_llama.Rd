% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/quick_llama.R
\name{quick_llama}
\alias{quick_llama}
\title{Quick LLaMA Inference}
\usage{
quick_llama(
  prompt,
  model = .get_default_model(),
  n_threads = NULL,
  n_gpu_layers = "auto",
  n_ctx = 2048L,
  max_tokens = 100L,
  temperature = 0.7,
  top_p = 0.9,
  top_k = 40L,
  repeat_penalty = 1.1,
  min_p = 0.05,
  stream = NULL,
  seed = 1234L,
  ...
)
}
\arguments{
\item{prompt}{Character string or vector of prompts to process}

\item{model}{Model URL or path (default: Llama-3.2-1B-Instruct Q4_K_M)}

\item{n_threads}{Number of threads (default: auto-detect)}

\item{n_gpu_layers}{Number of GPU layers (default: auto-detect)}

\item{n_ctx}{Context size (default: 2048)}

\item{max_tokens}{Maximum tokens to generate (default: 100)}

\item{temperature}{Sampling temperature (default: 0.7)}

\item{top_p}{Top-p sampling (default: 0.9)}

\item{top_k}{Top-k sampling (default: 40)}

\item{repeat_penalty}{Repetition penalty (default: 1.1)}

\item{min_p}{Minimum probability threshold (default: 0.05)}

\item{stream}{Whether to stream output (default: auto-detect based on interactive())}

\item{seed}{Random seed for reproducibility (default: 1234)}

\item{...}{Additional parameters passed to generate() or generate_parallel()}
}
\value{
Character string (single prompt) or named list (multiple prompts)
}
\description{
A high-level convenience function that provides one-line LLM inference.
Automatically handles model downloading, loading, and text generation.
}
\details{
This function provides a zero-configuration approach to using LLaMA models:

\itemize{
\item \strong{Single prompt}: Returns a character string with the generated response
\item \strong{Multiple prompts}: Returns a named list with responses for each prompt
\item \strong{Auto-detection}: Automatically detects GPU support and optimal threading
\item \strong{Model caching}: Downloads and caches models for faster subsequent use
\item \strong{Smart defaults}: Uses recommended parameters optimized for quality and speed
}

The function handles the complete workflow:
\enumerate{
\item Backend initialization (if needed)
\item Model download and caching (if needed)
\item Model loading and context creation
\item Text generation with optimal parameters
}

Default model is Llama-3.2-1B-Instruct Q4_K_M (~0.8GB), which provides good quality
while being suitable for 8GB RAM systems running CPU-only inference.
}
\examples{
\dontrun{
# Simple usage - single prompt
response <- quick_llama("Hello, how are you?")
print(response)

# Multiple prompts - batch processing
prompts <- c("Summarize machine learning", 
             "Explain quantum computing", 
             "What is artificial intelligence?")
responses <- quick_llama(prompts)
print(responses)

# Custom parameters for creative writing
story <- quick_llama("Tell me a short story about a robot", 
                     temperature = 0.9, 
                     max_tokens = 200)
print(story)

# Using a different model
response <- quick_llama("Translate to Spanish: Hello world",
                        model = "https://example.com/other-model.gguf")

# Reproducible results with seed
response1 <- quick_llama("Generate a random number", seed = 42)
response2 <- quick_llama("Generate a random number", seed = 42)
# response1 and response2 will be identical
}
}
\seealso{
\code{\link{quick_llama_reset}} to clear cached models,
\code{\link{model_load}} and \code{\link{generate}} for low-level control,
\code{\link{install_newrllama}} for backend installation
}