# newrllama4

[![R-CMD-check](https://github.com/xu2009/newrllama4-project/workflows/R-CMD-check/badge.svg)](https://github.com/xu2009/newrllama4-project/actions)
[![codecov](https://codecov.io/gh/xu2009/newrllama4-project/branch/main/graph/badge.svg)](https://codecov.io/gh/xu2009/newrllama4-project)
[![CRAN status](https://www.r-pkg.org/badges/version/newrllama4)](https://CRAN.R-project.org/package=newrllama4)

**R Interface to Large Language Models with Runtime Library Loading**

`newrllama4` is an R package that provides access to large language models (LLMs) through the high-performance llama.cpp backend. The package is designed for researchers and data analysts who want to integrate language model capabilities into their R workflows without complex setup requirements.

---

## Quick Start

Get started with large language models in R in three steps:

```r
# Step 1: Install the R package
devtools::install_github("xu2009/newrllama4-project", subdir = "newrllama4")

# Step 2: Download the backend library (first-time setup)
library(newrllama4)
install_newrllama()

# Step 3: Generate text using a language model
response <- quick_llama("Explain the concept of statistical significance")
print(response)
```

---

## Introduction to Large Language Models

Large Language Models (LLMs) are neural networks trained on vast amounts of text data that can understand and generate human language. Key concepts:

- **Large Language Models**: AI systems that process and generate text based on patterns learned from training data
- **GGUF Format**: An efficient binary format for storing and loading language models, optimized for inference
- **Text Generation**: The process of producing coherent text responses based on input prompts

### Research Applications

- **Text Analysis**: Automated content analysis, sentiment analysis, and text classification
- **Data Documentation**: Generate descriptions and summaries of datasets and analytical results  
- **Code Generation**: Produce R code snippets and analysis templates
- **Literature Review**: Summarize research papers and extract key findings
- **Survey Analysis**: Process and categorize open-ended survey responses

---

## Installation

### System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **R Version** | R â‰¥ 4.0 | R â‰¥ 4.2 |
| **Memory** | 4GB RAM | 8GB+ RAM |
| **Storage** | 2GB available | 5GB+ available |
| **Network** | Required for downloads | Stable connection |

### Supported Platforms

- **macOS**: Apple Silicon (M1/M2/M3) and Intel processors
- **Windows**: 64-bit systems
- **Linux**: 64-bit distributions (Ubuntu, CentOS, etc.)

### Installation Steps

#### Step 1: Install R Package

```r
# Method 1: Install from GitHub (recommended)
if (!require(devtools)) install.packages("devtools")
devtools::install_github("xu2009/newrllama4-project", subdir = "newrllama4")

# Method 2: From CRAN (when available)
# install.packages("newrllama4")
```

#### Step 2: Download Backend Library

```r
library(newrllama4)

# Automatically detects your system and downloads the appropriate backend (~1MB)
install_newrllama()
```

#### Step 3: Verify Installation

```r
# Check if installation was successful
if (lib_is_installed()) {
  message("Installation successful!")
} else {
  message("Installation failed. See troubleshooting section.")
}
```

### Common Installation Issues

| Issue | Solution |
|-------|----------|
| **Network connection failed** | Check firewall settings or try using a VPN |
| **Insufficient permissions** | Run R as administrator/with elevated privileges |
| **Insufficient disk space** | Free up disk space (minimum 2GB required) |
| **R version too old** | Update to R 4.0 or higher |

---

## Core Functionality

### Level 1: Basic Usage with `quick_llama()`

The simplest way to use the package for text generation:

```r
library(newrllama4)

# Basic text generation
response <- quick_llama("What is machine learning?")
print(response)

# Creative writing with higher temperature
story <- quick_llama("Write a short story about a data scientist", 
                     temperature = 0.9,  # More creative output
                     max_tokens = 200)   # Longer response

# Process multiple questions
questions <- c("Explain regression analysis", 
               "What is clustering?", 
               "Recommend data visualization tools")
answers <- quick_llama(questions)
```

#### Parameter Tuning Guidelines

```r
# Precise answers (suitable for factual questions)
precise <- quick_llama("What is the formula for standard deviation?", 
                       temperature = 0.1,  # Low temperature = more accurate
                       max_tokens = 50)

# Creative responses (suitable for open-ended tasks)
creative <- quick_llama("Design a data analysis project", 
                        temperature = 0.8,  # High temperature = more creative
                        max_tokens = 300)

# Batch processing (for multiple tasks)
prompts <- paste("Analyze the characteristics of dataset", 1:10)
results <- quick_llama(prompts, max_tokens = 100)
```

### Level 2: Intermediate Control - Model Management

When you need more control over the generation process:

```r
# Load a specific model (will download if needed)
model <- model_load("https://huggingface.co/microsoft/DialoGPT-medium/model.gguf")

# Create inference context with custom settings
ctx <- context_create(model, 
                      n_ctx = 4096,     # Longer conversation memory
                      n_threads = 8)    # Use more CPU cores

# Manual text generation
tokens <- tokenize(model, "Please explain deep learning")
result <- generate(ctx, tokens, max_tokens = 150)
text <- detokenize(model, result)
```

### Level 3: Advanced Features

#### Multi-turn Conversations

```r
# Load a chat-optimized model
model <- model_load("path/to/chat_model.gguf")

# Build conversation history
messages <- list(
  list(role = "system", content = "You are a professional data science consultant"),
  list(role = "user", content = "I want to learn R programming"),
  list(role = "assistant", content = "Great choice! R is excellent for statistical analysis..."),
  list(role = "user", content = "Please recommend some beginner books")
)

# Apply chat template formatting
formatted_prompt <- apply_chat_template(model, messages)
response <- quick_llama(formatted_prompt)
```

#### GPU Acceleration Setup

```r
# Enable GPU acceleration (if available)
gpu_model <- model_load("model.gguf", 
                        n_gpu_layers = -1,  # Use all GPU layers
                        use_mlock = TRUE)   # Lock model in memory

# Create context optimized for GPU
ctx_gpu <- context_create(gpu_model, 
                          n_ctx = 8192,     # Larger context window
                          n_threads = 4)    # Fewer CPU threads when using GPU
```

### Level 4: Research Applications

#### Data Analysis Assistant

```r
# Create a data analysis helper function
analyze_data <- function(dataset_description) {
  prompt <- paste("As a data scientist, please analyze the following dataset:", 
                  dataset_description,
                  "Please provide: 1) Data characteristics 2) Analysis recommendations 3) Potential issues")
  
  return(quick_llama(prompt, 
                     temperature = 0.3,  # Maintain analytical rigor
                     max_tokens = 300))
}

# Usage example
result <- analyze_data("E-commerce purchase data with 1000 users, including age, gender, and purchase amount variables")
```

#### R Code Generation

```r
# R code assistant function
generate_r_code <- function(task_description) {
  prompt <- paste("Generate R code to complete the following task:", 
                  task_description,
                  "Return only executable R code with necessary comments.")
  
  return(quick_llama(prompt, temperature = 0.2))
}

# Example usage
code <- generate_r_code("Create a scatter plot showing the relationship between height and weight, including a regression line")
cat(code)
```

---

## Real-world Applications

### 1. Literature Review Assistant

```r
# Literature review helper
literature_review <- function(topic, keywords) {
  prompt <- paste("Create a literature review outline for the following topic:", topic,
                  "Keywords:", paste(keywords, collapse = ", "))
  return(quick_llama(prompt, max_tokens = 400))
}

outline <- literature_review("Machine learning applications in medical diagnosis", 
                           c("deep learning", "medical imaging", "diagnostic accuracy"))
```

### 2. Automated Report Generation

```r
# Generate analysis reports
generate_report <- function(data_summary) {
  prompt <- paste("Generate a professional analysis report based on the following data summary:", 
                  data_summary,
                  "Include key findings, trend analysis, and recommendations.")
  return(quick_llama(prompt, temperature = 0.4, max_tokens = 500))
}
```

### 3. Teaching Assistant

```r
# Statistical concept explainer
explain_concept <- function(concept, level = "beginner") {
  prompt <- paste("Explain the statistical concept", concept, "at a", level, "level.",
                  "Use simple language and practical examples.")
  return(quick_llama(prompt, temperature = 0.3))
}

explanation <- explain_concept("p-value", "beginner")
```

---

## Performance Optimization

### Speed Optimization

```r
# Optimized configuration example
optimized_model <- model_load(
  model_path = "model.gguf",
  n_gpu_layers = -1,        # Full GPU acceleration
  use_mmap = TRUE,          # Memory mapping
  use_mlock = TRUE,         # Lock in memory
  check_memory = TRUE       # Memory checking
)

optimized_ctx <- context_create(
  model = optimized_model,
  n_ctx = 4096,             # Moderate context length
  n_threads = parallel::detectCores() - 1  # Use most CPU cores
)
```

### Memory Management

| Configuration | Memory Usage | Speed | Recommended For |
|---------------|--------------|-------|----------------|
| **CPU Only** | Low | Medium | General tasks, memory constrained |
| **Partial GPU** | Medium | High | Balanced performance and memory |
| **Full GPU** | High | Highest | High-performance requirements |

```r
# Memory-efficient configuration
memory_efficient <- model_load("model.gguf", 
                               n_gpu_layers = 0,     # CPU only
                               use_mmap = TRUE,      # Reduce memory usage
                               use_mlock = FALSE)

# High-performance configuration (requires sufficient memory)
high_performance <- model_load("model.gguf",
                               n_gpu_layers = -1,    # Full GPU
                               use_mlock = TRUE)     # Maximum speed
```

### Parameter Tuning Guidelines

#### Temperature Setting Guide

- **0.1-0.3**: Academic and professional answers (high factual accuracy)
- **0.4-0.6**: Business and daily conversations (balanced)
- **0.7-0.9**: Creative writing and brainstorming
- **1.0+**: Highly creative output (may be incoherent)

#### Max Tokens Selection

- **50-100**: Brief answers and summaries
- **100-300**: Standard explanations and analysis
- **300-500**: Detailed reports and long-form content
- **500+**: Articles and in-depth analysis

---

## Troubleshooting

### Common Errors and Solutions

#### "Backend library not found"

**Cause**: Backend engine not properly installed

**Solution**:
```r
# Reinstall backend
install_newrllama()

# Check installation status
lib_is_installed()

# View library path
get_lib_path()
```

#### "Model loading failed"

**Cause**: Corrupted model file or network issues

**Solution**:
```r
# Force re-download of model
model <- model_load("model_url", force_redownload = TRUE)

# Check disk space
file.info(get_model_cache_dir())
```

#### "Out of memory"

**Cause**: Insufficient system memory

**Solution**:
```r
# Use smaller context size
ctx <- context_create(model, n_ctx = 1024)  # Reduce to 1024

# Close other programs to free memory
# Or use memory-friendly configuration
model <- model_load("model.gguf", n_gpu_layers = 0)
```

### ğŸ› è°ƒè¯•æŠ€å·§

```r
# å¯ç”¨è¯¦ç»†æ—¥å¿—
options(newrllama.verbose = TRUE)

# æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
Sys.info()

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
memory.size()  # Windows
object.size(model)  # æ£€æŸ¥æ¨¡å‹å¤§å°
```

### Getting Help

1. **Documentation**: `?quick_llama`
2. **Report Issues**: [GitHub Issues](https://github.com/xu2009/newrllama4-project/issues)
3. **Community Discussion**: [GitHub Discussions](https://github.com/xu2009/newrllama4-project/discussions)

---

## ğŸ“Š æ¨èæ¨¡å‹æŒ‡å—

### ğŸ’» æŒ‰ç”¨é€”é€‰æ‹©æ¨¡å‹

| ç”¨é€” | æ¨èæ¨¡å‹ | å¤§å° | ç‰¹ç‚¹ |
|------|----------|------|------|
| **å¿«é€Ÿæµ‹è¯•** | Llama-3.2-1B-Instruct | ~1GB | é€Ÿåº¦å¿«ï¼ŒåŸºç¡€åŠŸèƒ½ |
| **æ—¥å¸¸å¯¹è¯** | Llama-3.2-3B-Instruct | ~2GB | å¹³è¡¡æ€§èƒ½ |
| **ä¸“ä¸šä»»åŠ¡** | Llama-3.1-8B-Instruct | ~5GB | é«˜è´¨é‡å›ç­” |
| **ä»£ç ç”Ÿæˆ** | CodeLlama-7B-Instruct | ~4GB | ç¼–ç¨‹ä¸“ç”¨ |

### ğŸ“ˆ æ€§èƒ½vsè´¨é‡æƒè¡¡

```r
# å¿«é€ŸåŸå‹ - ä¼˜å…ˆé€Ÿåº¦
quick_model <- model_load("https://huggingface.co/.../Llama-3.2-1B-Instruct-Q4_K_M.gguf")

# ç”Ÿäº§ç¯å¢ƒ - ä¼˜å…ˆè´¨é‡
production_model <- model_load("https://huggingface.co/.../Llama-3.1-8B-Instruct-Q4_K_M.gguf")

# ç¦»çº¿ä½¿ç”¨ - æœ¬åœ°æ–‡ä»¶
local_model <- model_load("/path/to/your/model.gguf")
```

### ğŸŒ å¤šè¯­è¨€æ”¯æŒ

```r
# ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
chinese_model <- quick_llama("è¯·ç”¨ä¸­æ–‡å›ç­”ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")

# å¤šè¯­è¨€å¤„ç†
multilingual <- quick_llama(c(
  "Explain AI in English",
  "Explique l'IA en franÃ§ais", 
  "ç”¨ä¸­æ–‡è§£é‡Šäººå·¥æ™ºèƒ½"
))
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

newrllama4 é‡‡ç”¨åˆ›æ–°çš„å››å±‚æ¶æ„ï¼Œå¹³è¡¡äº†æ˜“ç”¨æ€§å’Œæ€§èƒ½ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        é«˜çº§Ræ¥å£ (quick_llama)       â”‚  â† ç”¨æˆ·ç›´æ¥ä½¿ç”¨
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ä¸­çº§API (model_load, generate)  â”‚  â† é«˜çº§ç”¨æˆ·
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      R/C++æ¡¥æ¥å±‚ (Rcppæ¥å£)        â”‚  â† æ•°æ®è½¬æ¢
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        C++åç«¯ (llama.cpp)          â”‚  â† æ ¸å¿ƒè®¡ç®—å¼•æ“
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

- **ğŸ¯ æ˜“ç”¨æ€§**: `quick_llama()` ä¸€è¡Œä»£ç å³å¯ä½¿ç”¨
- **âš¡ æ€§èƒ½**: åº•å±‚C++å¼•æ“ç¡®ä¿æœ€é«˜æ•ˆç‡  
- **ğŸ“¦ è½»é‡çº§**: è¿è¡Œæ—¶ä¸‹è½½é¿å…å¤§åŒ…ä½“ç§¯
- **ğŸ”§ çµæ´»æ€§**: æ”¯æŒä»ç®€å•åˆ°å¤æ‚çš„å„ç§ç”¨æ³•

---

## ğŸ”¬ é«˜çº§ä¸»é¢˜

### è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒ

è™½ç„¶æœ¬åŒ…ä¸»è¦ç”¨äºæ¨ç†ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„GGUFæ¨¡å‹ï¼š

```r
# åŠ è½½æ‚¨è‡ªå·±è®­ç»ƒçš„æ¨¡å‹
custom_model <- model_load("/path/to/your/fine-tuned-model.gguf")

# æˆ–ä»Hugging FaceåŠ è½½
hf_model <- model_load("hf://your-username/your-model/model.gguf")
```

### Batch Processing Best Practices

```r
# Efficient batch processing
process_batch <- function(prompts, batch_size = 10) {
  results <- list()
  
  for (i in seq(1, length(prompts), batch_size)) {
    batch <- prompts[i:min(i + batch_size - 1, length(prompts))]
    batch_results <- quick_llama(batch)
    results <- c(results, batch_results)
    
    # Progress indicator
    message("Progress: ", min(i + batch_size - 1, length(prompts)), "/", length(prompts))
  }
  
  return(results)
}
```

### ä¸å…¶ä»–RåŒ…é›†æˆ

```r
# ä¸ggplot2ç»“åˆ - AIè¾…åŠ©å¯è§†åŒ–
library(ggplot2)

plot_suggestion <- quick_llama(
  "åŸºäºirisæ•°æ®é›†ï¼Œå»ºè®®ä¸€ä¸ªæœ‰æ„ä¹‰çš„å¯è§†åŒ–æ–¹æ¡ˆï¼Œè¿”å›ggplot2ä»£ç "
)

# ä¸dplyrç»“åˆ - æ•°æ®å¤„ç†å»ºè®®
library(dplyr)

data_analysis <- quick_llama(
  "ä¸ºé”€å”®æ•°æ®åˆ†ææä¾›dplyrç®¡é“æ“ä½œæ­¥éª¤"
)
```

---

## ğŸ¤ ç¤¾åŒºä¸è´¡çŒ®

### å‚ä¸è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› **æŠ¥å‘ŠBug**: [æäº¤Issue](https://github.com/xu2009/newrllama4-project/issues/new?template=bug_report.md)
- ğŸ’¡ **åŠŸèƒ½å»ºè®®**: [åŠŸèƒ½è¯·æ±‚](https://github.com/xu2009/newrllama4-project/issues/new?template=feature_request.md)
- ğŸ“– **æ”¹è¿›æ–‡æ¡£**: æäº¤PRæ”¹è¿›READMEå’Œæ–‡æ¡£
- ğŸ’» **ä»£ç è´¡çŒ®**: Forké¡¹ç›®å¹¶æäº¤Pull Request

### å¼€å‘ç¯å¢ƒè®¾ç½®

```r
# å¼€å‘ç‰ˆæœ¬å®‰è£…
devtools::install_github("xu2009/newrllama4-project", 
                         subdir = "newrllama4", 
                         ref = "develop")

# è¿è¡Œæµ‹è¯•
devtools::test()

# æ„å»ºæ–‡æ¡£
devtools::document()
```

### è¡Œä¸ºå‡†åˆ™

æˆ‘ä»¬è‡´åŠ›äºåˆ›å»ºä¸€ä¸ªå‹å¥½ã€åŒ…å®¹çš„å¼€æºç¤¾åŒºã€‚è¯·é˜…è¯»æˆ‘ä»¬çš„[è¡Œä¸ºå‡†åˆ™](https://github.com/xu2009/newrllama4-project/blob/main/CODE_OF_CONDUCT.md)ã€‚

---

## ğŸ“š ç›¸å…³èµ„æº

### å­¦ä¹ èµ„æº

- ğŸ“– **Rè¯­è¨€**: [R for Data Science](https://r4ds.had.co.nz/)
- ğŸ¤– **AIåŸºç¡€**: [Elements of AI](https://www.elementsofai.com/)
- ğŸ§  **æ·±åº¦å­¦ä¹ **: [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r)

### ç›¸å…³é¡¹ç›®

- ğŸ¦™ **llama.cpp**: [åŸå§‹C++é¡¹ç›®](https://github.com/ggerganov/llama.cpp)
- ğŸ¤— **Hugging Face**: [æ¨¡å‹åº“](https://huggingface.co/models)
- ğŸ“Š **Rè¯­è¨€**: [å®˜æ–¹ç½‘ç«™](https://www.r-project.org/)

### æŠ€æœ¯åšå®¢

- [åœ¨Rä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æœ€ä½³å®è·µ](https://github.com/xu2009/newrllama4-project/wiki)
- [æ€§èƒ½è°ƒä¼˜æŒ‡å—](https://github.com/xu2009/newrllama4-project/wiki/Performance-Tuning)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## â­ æ”¯æŒé¡¹ç›®

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ï¼š

- â­ ç»™é¡¹ç›®åŠ æ˜Ÿæ ‡
- ğŸ› æŠ¥å‘Šé—®é¢˜å’Œå»ºè®®
- ğŸ“¢ æ¨èç»™æœ‹å‹å’ŒåŒäº‹
- ğŸ’ [æˆä¸ºèµåŠ©è€…](https://github.com/sponsors/xu2009)

---

## ğŸ“ è”ç³»æˆ‘ä»¬

- ğŸ“§ **é‚®ç®±**: yaoshengleo@example.com
- ğŸ™ **GitHub**: [@xu2009](https://github.com/xu2009)
- ğŸ¦ **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/xu2009/newrllama4-project/issues)

---

<div align="center">

**è®©AIåœ¨Rä¸­å˜å¾—ç®€å•è€Œå¼ºå¤§** ğŸš€

[å¿«é€Ÿå¼€å§‹](#-quick-start---30ç§’ä¸Šæ‰‹) â€¢ [æŸ¥çœ‹æ–‡æ¡£](https://github.com/xu2009/newrllama4-project/wiki) â€¢ [æŠ¥å‘Šé—®é¢˜](https://github.com/xu2009/newrllama4-project/issues)

</div>