library(newrllama4)
# 2. 检查并安装后端库
cat("⬇️  [2/5] 检查预编译后端库...\n")
if (!lib_is_installed()) {
cat("    正在下载预编译库...\n")
install_newrllama()
} else {
cat("    ✅ 后端库已安装\n")
}
# 3. 初始化后端
cat("🔧 [3/5] 初始化后端...\n")
backend_init()
# 4. 加载模型
cat("📚 [4/5] 加载 Llama 模型...\n")
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("❌ 请更新模型路径\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    ✅ 模型加载成功 (Metal GPU 加速)\n")
# 创建推理上下文
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# 单序列生成
cat("═══ 单序列文本生成 ═══\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("输入: %s\n", prompt))
cat(sprintf("输出: %s\n\n", result))
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
)
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting.",
)
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s → %s\n", i, prompts[i], results[i]))
}
cat("\n🎉 演示完成！所有功能正常工作\n")
# 清理
backend_free()
gc()
