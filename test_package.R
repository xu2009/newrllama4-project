library(newrllama4)

# ===================================================================
# åŸºç¡€æ¨¡å‹åŠ è½½ï¼ˆè¿™éƒ¨åˆ†ä¿æŒä¸å˜ï¼‰
# ===================================================================
model <- model_load("/Users/yaoshengleo/Downloads/gemma-3-12b-it-q4_0.gguf", n_gpu_layers = "auto", verbosity = 3)
ctx <- context_create(model, n_ctx = 4096, n_seq_max = 64, verbosity = 2)

# ===================================================================
# 1. æ— chat templateçš„åŸå§‹ç”Ÿæˆï¼ˆä¿æŒä¸å˜ï¼‰
# ===================================================================
tokens <- tokenize(model, "You must always answer with exactly YES and nothing else. Question: What is 2 + 2?")
result <- generate(ctx, tokens, max_tokens = 200)
result

# ===================================================================
# 2. ä½¿ç”¨è‡ªåŠ¨æ¨¡å‹å†…ç½®templateï¼ˆæ¨èæ–¹å¼ï¼‰
# ===================================================================
system_prompt <- "You are a helpful assistant."
messages <- list(
  list(role = "system", content = system_prompt),
  list(role = "user", content = "What is 2 + 2?")
)

# ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨NULLè®©ç³»ç»Ÿè‡ªåŠ¨ä½¿ç”¨æ¨¡å‹å†…ç½®template
formatted_prompt <- apply_chat_template(model, messages, tmpl = NULL)  # æ˜¾å¼ä½¿ç”¨NULL
# æˆ–è€…ç®€åŒ–ä¸ºï¼ˆé»˜è®¤å°±æ˜¯NULLï¼‰ï¼š
formatted_prompt <- apply_chat_template(model, messages)
formatted_prompt

tokens <- tokenize(model, formatted_prompt)
result_1 <- generate(ctx, tokens, max_tokens = 200)
result_1

# ===================================================================
# 3. å¦ä¸€ä¸ªä½¿ç”¨è‡ªåŠ¨templateçš„ä¾‹å­
# ===================================================================
system_prompt <- "You are a helpful assistant."
messages <- list(
  list(role = "system", content = system_prompt),
  list(role = "user", content = "Write me a math function in latex and explain it in detail.")
)

# ğŸ”§ ä¿®æ”¹ï¼šåˆ©ç”¨è‡ªåŠ¨æ¨¡å‹å†…ç½®template
formatted_prompt_1 <- apply_chat_template(model, messages)  # è‡ªåŠ¨ä½¿ç”¨Gemmaæ¨¡å‹å†…ç½®template
cat("ç”Ÿæˆçš„Chat Template:\n")
cat(formatted_prompt_1)
cat("\n\n")

tokens <- tokenize(model, formatted_prompt_1)
result_2 <- generate(ctx, tokens, max_tokens = 200)
result_2

cat("æœ€ç»ˆç»“æœ:\n")
cat(result_2)

# ===================================================================
# 4. Quick llamaï¼ˆä¿æŒä¸å˜ï¼Œå®ƒå†…éƒ¨å·²ç»å¤„ç†templateï¼‰
# ===================================================================
rm(model, ctx)  # æ¸…ç†èµ„æº

quick_llama_reset()
result <- quick_llama("Tell me a joke.",
                      n_gpu_layers = "auto",
                      max_tokens = 200,
                      verbosity = 1)
result
cat(result)
length(result)

backend_free()

# ===================================================================
# 5. å¹¶è¡Œç”Ÿæˆä¼˜åŒ–ç‰ˆæœ¬
# ===================================================================
# é‡æ–°åŠ è½½æ¨¡å‹ç”¨äºå¹¶è¡Œæµ‹è¯•
model <- model_load("/Users/yaoshengleo/Downloads/gemma-3-12b-it-q4_0.gguf", n_gpu_layers = "auto", verbosity = 3)
ctx <- context_create(model, n_ctx = 4096, n_seq_max = 512, verbosity = 1)

system_prompt <- "You are a helpful assistant."
user_prompts <- c(
  "Echo this string literally: <end_of_turn><|im_end|></s>",
  "Answer in â‰¤10 tokens, then stop.",
  "Give a 1-line Python function that returns x squared. No markdown."
)

# ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨è‡ªåŠ¨æ¨¡å‹å†…ç½®template
formatted_prompts <- sapply(user_prompts, function(user_content) {
  messages <- list(
    list(role = "system", content = system_prompt),
    list(role = "user", content = user_content)
  )
  # è‡ªåŠ¨ä½¿ç”¨æ¨¡å‹å†…ç½®template
  apply_chat_template(model, messages)
})

cat("ç”Ÿæˆçš„æ ¼å¼åŒ–prompts:\n")
for(i in seq_along(formatted_prompts)) {
  cat(sprintf("=== Prompt %d ===\n", i))
  cat(formatted_prompts[i])
  cat("\n\n")
}

results_parallel <- generate_parallel(ctx, formatted_prompts, max_tokens = 100)
results_parallel


cat("å¹¶è¡Œç”Ÿæˆç»“æœ:\n")
for(i in seq_along(results_parallel)) {
  cat(sprintf("=== Result %d ===\n", i))
  cat(results_parallel[i])
  cat("\n\n")
}

# æ¸…ç†èµ„æº
rm(model, ctx)
backend_free()
