// =============================================================================
// å®Œæ•´æ”¹è¿›çš„å¹¶è¡Œç”Ÿæˆå‡½æ•° - é«˜æ•ˆä¸”å®‰å…¨çš„å®ç°
// =============================================================================

NEWRLLAMA_API newrllama_error_code newrllama_generate_parallel_improved(
    newrllama_context_handle ctx, 
    const char** prompts, 
    int n_prompts, 
    const struct newrllama_parallel_params* params, 
    char*** results_out, 
    const char** error_message) {
    
    // =============================================================================
    // Phase 1: å‚æ•°éªŒè¯å’Œåˆå§‹åŒ–
    // =============================================================================
    
    if (!ctx || !params || !prompts || !results_out || n_prompts <= 0) {
        set_error(error_message, "Invalid parameters: null pointers or invalid prompt count");
        return NEWRLLAMA_ERROR;
    }
    
    const llama_model* model = llama_get_model(ctx);
    const llama_vocab* vocab = llama_model_get_vocab(model);
    const llama_token eos_token = llama_vocab_eos(vocab);
    const int n_ctx = llama_n_ctx(ctx);
    
    // æ€§èƒ½ç»Ÿè®¡
    const auto t_start = ggml_time_us();
    int32_t n_cache_miss = 0;
    int32_t n_total_prompt = 0;
    int32_t n_total_gen = 0;
    
    // å®¢æˆ·ç«¯ç»“æ„ä½“ - å¢å¼ºç‰ˆ
    struct EnhancedClient {
        int id;
        llama_seq_id seq_id;
        std::vector<llama_token> prompt_tokens;
        int32_t n_past = 0;
        int32_t n_prompt = 0;
        int32_t n_decoded = 0;
        int32_t i_batch = -1;
        std::string input;
        std::string response;
        llama_token sampled = 0;
        common_sampler* smpl = nullptr;
        bool finished = false;
        bool failed = false;
        std::string error_msg;
        int64_t t_start_prompt = 0;
        int64_t t_start_gen = 0;
        
        ~EnhancedClient() {
            if (smpl) {
                common_sampler_free(smpl);
                smpl = nullptr;
            }
        }
    };
    
    std::vector<EnhancedClient> clients(n_prompts);
    
    // =============================================================================
    // Phase 2: å®¢æˆ·ç«¯åˆå§‹åŒ–å’Œä»¤ç‰ŒåŒ–
    // =============================================================================
    
    try {
        // é…ç½®é‡‡æ ·å‚æ•°
        common_params_sampling sparams{};
        sparams.top_k = params->top_k;
        sparams.top_p = params->top_p;
        sparams.temp = params->temperature;
        sparams.penalty_last_n = params->repeat_last_n;
        sparams.penalty_repeat = params->penalty_repeat;
        sparams.seed = (params->seed < 0) ? time(nullptr) : params->seed;
        
        // æ¸…ç©ºKVç¼“å­˜ - ç¡®ä¿å¹²å‡€çš„å¼€å§‹
        llama_kv_self_clear(ctx);
        
        // åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯
        for (int i = 0; i < n_prompts; ++i) {
            auto& client = clients[i];
            client.id = i;
            client.seq_id = i + 1;  // åºåˆ—IDä»1å¼€å§‹ï¼Œé¿å…ä¸ç³»ç»Ÿåºåˆ—å†²çª
            client.input = std::string(prompts[i]);
            client.t_start_prompt = ggml_time_us();
            
            // ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ›å»ºç‹¬ç«‹çš„é‡‡æ ·å™¨
            client.smpl = common_sampler_init(model, sparams);
            if (!client.smpl) {
                throw std::runtime_error("Failed to initialize sampler for client " + std::to_string(i));
            }
            
            // ä»¤ç‰ŒåŒ–ç”¨æˆ·è¾“å…¥
            client.prompt_tokens = helper_tokenize(model, client.input, true);
            client.n_prompt = client.prompt_tokens.size();
            n_total_prompt += client.n_prompt;
            
            // éªŒè¯æç¤ºç¬¦é•¿åº¦
            if (client.n_prompt > n_ctx - 64) {  // é¢„ç•™ç”Ÿæˆç©ºé—´
                client.failed = true;
                client.error_msg = "Prompt too long for context size";
                continue;
            }
        }
        
        // =============================================================================
        // Phase 3: ç»Ÿä¸€æ‰¹æ¬¡å¤„ç†æ‰€æœ‰æç¤ºç¬¦ - å…³é”®æ”¹è¿›
        // =============================================================================
        
        // è®¡ç®—æœ€å¤§æ‰¹æ¬¡å¤§å°
        int max_batch_size = 0;
        for (const auto& client : clients) {
            if (!client.failed) {
                max_batch_size += client.n_prompt;
            }
        }
        
        if (max_batch_size == 0) {
            throw std::runtime_error("All clients failed during initialization");
        }
        
        // åˆ›å»ºç»Ÿä¸€æ‰¹æ¬¡ - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æç¤ºç¬¦
        llama_batch batch = llama_batch_init(std::max(max_batch_size, n_ctx), 0, n_prompts);
        
        // ğŸ”‘ å…³é”®ï¼šæ‰€æœ‰å®¢æˆ·ç«¯çš„æç¤ºç¬¦æ·»åŠ åˆ°åŒä¸€æ‰¹æ¬¡ï¼Œä½†ä½¿ç”¨ç‹¬ç«‹çš„seq_id
        for (auto& client : clients) {
            if (client.failed) continue;
            
            for (size_t j = 0; j < client.prompt_tokens.size(); ++j) {
                // æ¯ä¸ªå®¢æˆ·ç«¯ä½¿ç”¨ç‹¬ç«‹çš„seq_idï¼Œç¡®ä¿å®Œå…¨éš”ç¦»
                common_batch_add(batch, 
                               client.prompt_tokens[j], 
                               j,                           // position in sequence
                               {client.seq_id},            // ğŸ”‘ ç‹¬ç«‹åºåˆ—ID
                               j == client.prompt_tokens.size() - 1);  // åªæœ‰æœ€åä¸€ä¸ªtokenéœ€è¦logits
            }
            client.n_past = client.n_prompt;
        }
        
        // åˆ†å—å¤„ç†æ‰¹æ¬¡ - å¤„ç†ä¸Šä¸‹æ–‡é™åˆ¶
        const int32_t n_batch_max = 512;  // æœ€å¤§æ‰¹æ¬¡å¤§å°
        for (int32_t i = 0; i < (int32_t)batch.n_tokens; i += n_batch_max) {
            const int32_t n_tokens = std::min(n_batch_max, (int32_t)(batch.n_tokens - i));
            
            llama_batch batch_view = {
                n_tokens,
                batch.token + i,
                nullptr,
                batch.pos + i,
                batch.n_seq_id + i,
                batch.seq_id + i,
                batch.logits + i,
            };
            
            const int ret = llama_decode(ctx, batch_view);
            if (ret != 0) {
                llama_batch_free(batch);
                
                // ğŸ›¡ï¸ é”™è¯¯å¤„ç†ï¼šå¦‚æœç»Ÿä¸€æ‰¹æ¬¡å¤±è´¥ï¼Œå›é€€åˆ°ç‹¬ç«‹å¤„ç†
                if (ret < 0) {
                    throw std::runtime_error("Fatal decode error in prompt processing: " + std::to_string(ret));
                }
                
                // éè‡´å‘½é”™è¯¯ï¼Œå°è¯•åˆ†åˆ«å¤„ç†æ¯ä¸ªå®¢æˆ·ç«¯
                for (auto& client : clients) {
                    if (client.failed) continue;
                    
                    llama_batch individual_batch = llama_batch_init(client.n_prompt, 0, 1);
                    
                    for (size_t j = 0; j < client.prompt_tokens.size(); ++j) {
                        common_batch_add(individual_batch, 
                                       client.prompt_tokens[j], 
                                       j, 
                                       {client.seq_id}, 
                                       j == client.prompt_tokens.size() - 1);
                    }
                    
                    if (llama_decode(ctx, individual_batch) != 0) {
                        client.failed = true;
                        client.error_msg = "Failed to decode individual prompt";
                    }
                    
                    llama_batch_free(individual_batch);
                }
                
                n_cache_miss += 1;
                break;
            }
        }
        
        if (batch.n_tokens > 0) {
            llama_batch_free(batch);
        }
        
        // =============================================================================
        // Phase 4: ç”Ÿæˆå¾ªç¯ - ç»Ÿä¸€æ‰¹æ¬¡ä½†ä¿æŒåºåˆ—éš”ç¦»
        // =============================================================================
        
        int active_clients = 0;
        for (const auto& client : clients) {
            if (!client.failed) active_clients++;
        }
        
        if (active_clients == 0) {
            throw std::runtime_error("No clients remain active after prompt processing");
        }
        
        // ç”Ÿæˆå¾ªç¯
        while (active_clients > 0) {
            // åˆ›å»ºç”Ÿæˆæ‰¹æ¬¡
            llama_batch gen_batch = llama_batch_init(n_ctx, 0, active_clients);
            std::vector<int> batch_to_client_map;
            batch_to_client_map.reserve(active_clients);
            
            // ä¸ºæ¯ä¸ªæ´»è·ƒå®¢æˆ·ç«¯æ·»åŠ å¾…ç”Ÿæˆçš„token
            for (auto& client : clients) {
                if (client.finished || client.failed) continue;
                
                // ç¬¬ä¸€æ¬¡ç”Ÿæˆæ—¶ï¼Œä»æç¤ºç¬¦çš„æœ€åä¸€ä¸ªtokenå¼€å§‹
                if (client.n_decoded == 0) {
                    client.sampled = client.prompt_tokens.back();
                    client.t_start_gen = ggml_time_us();
                }
                
                client.i_batch = gen_batch.n_tokens;
                const int pos = client.n_past + client.n_decoded;
                
                // ğŸ”‘ å…³é”®ï¼šæ¯ä¸ªå®¢æˆ·ç«¯ä½¿ç”¨ç‹¬ç«‹çš„seq_id
                common_batch_add(gen_batch, client.sampled, pos, {client.seq_id}, true);
                batch_to_client_map.push_back(client.id);
            }
            
            if (gen_batch.n_tokens == 0) {
                llama_batch_free(gen_batch);
                break;
            }
            
            // åˆ†å—å¤„ç†ç”Ÿæˆæ‰¹æ¬¡
            const int32_t n_batch_gen = std::min(n_batch_max, (int32_t)gen_batch.n_tokens);
            for (int32_t i = 0; i < (int32_t)gen_batch.n_tokens; i += n_batch_gen) {
                const int32_t n_tokens = std::min(n_batch_gen, (int32_t)(gen_batch.n_tokens - i));
                
                llama_batch batch_view = {
                    n_tokens,
                    gen_batch.token + i,
                    nullptr,
                    gen_batch.pos + i,
                    gen_batch.n_seq_id + i,
                    gen_batch.seq_id + i,
                    gen_batch.logits + i,
                };
                
                const int ret = llama_decode(ctx, batch_view);
                if (ret != 0) {
                    llama_batch_free(gen_batch);
                    
                    if (ret < 0) {
                        throw std::runtime_error("Fatal decode error in generation: " + std::to_string(ret));
                    }
                    
                    // éè‡´å‘½é”™è¯¯ï¼Œç»§ç»­å¤„ç†
                    n_cache_miss += 1;
                    break;
                }
                
                // ğŸ¯ å…³é”®ï¼šä¸ºæ¯ä¸ªå®¢æˆ·ç«¯ç‹¬ç«‹é‡‡æ ·
                for (int b = 0; b < (int)batch_to_client_map.size(); ++b) {
                    const int client_id = batch_to_client_map[b];
                    auto& client = clients[client_id];
                    
                    if (client.i_batch < i || client.i_batch >= i + n_tokens) continue;
                    
                    try {
                        // ğŸ”‘ å…³é”®ï¼šä½¿ç”¨å®¢æˆ·ç«¯ä¸“å±çš„é‡‡æ ·å™¨å’Œæ­£ç¡®çš„æ‰¹æ¬¡ä½ç½®
                        const int batch_pos = client.i_batch - i;
                        const llama_token new_token = common_sampler_sample(client.smpl, ctx, batch_pos);
                        common_sampler_accept(client.smpl, new_token, true);
                        
                        // æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
                        bool should_stop = false;
                        
                        // EOS tokenæ£€æŸ¥
                        if (new_token == eos_token || llama_vocab_is_eog(vocab, new_token)) {
                            should_stop = true;
                        }
                        
                        // æœ€å¤§tokenæ•°æ£€æŸ¥
                        if (params->max_tokens > 0 && client.n_decoded >= params->max_tokens) {
                            should_stop = true;
                        }
                        
                        // è½¬æ¢tokenä¸ºæ–‡æœ¬
                        const std::string token_str = common_token_to_piece(ctx, new_token);
                        client.response += token_str;
                        client.sampled = new_token;
                        client.n_decoded++;
                        
                        // å¯¹è¯ç»ˆæ­¢æ£€æŸ¥
                        if (client.n_decoded > 5 && 
                            (client.response.find("\n\nUser:") != std::string::npos || 
                             client.response.find("\n\nHuman:") != std::string::npos)) {
                            should_stop = true;
                        }
                        
                        if (should_stop) {
                            client.finished = true;
                            active_clients--;
                            n_total_gen += client.n_decoded;
                            
                            // ğŸ§¹ æ¸…ç†è¯¥å®¢æˆ·ç«¯çš„KVç¼“å­˜
                            llama_kv_self_seq_rm(ctx, client.seq_id, 0, -1);
                        }
                        
                    } catch (const std::exception& e) {
                        client.failed = true;
                        client.error_msg = "Sampling failed: " + std::string(e.what());
                        active_clients--;
                    }
                }
            }
            
            llama_batch_free(gen_batch);
        }
        
        // =============================================================================
        // Phase 5: ç»“æœå¤„ç†å’Œæ¸…ç†
        // =============================================================================
        
        // æ¸…ç†æ‰€æœ‰KVç¼“å­˜
        for (const auto& client : clients) {
            if (client.seq_id > 0) {
                llama_kv_self_seq_rm(ctx, client.seq_id, 0, -1);
            }
        }
        
        // æ€§èƒ½ç»Ÿè®¡
        const auto t_end = ggml_time_us();
        const double total_time = (t_end - t_start) / 1e6;
        
        // å‡†å¤‡ç»“æœ
        *results_out = new char*[n_prompts];
        for (int i = 0; i < n_prompts; ++i) {
            const auto& client = clients[i];
            
            if (client.failed) {
                // å¤±è´¥çš„å®¢æˆ·ç«¯è¿”å›é”™è¯¯ä¿¡æ¯
                std::string error_result = "[ERROR] " + client.error_msg;
                (*results_out)[i] = string_to_c_str(error_result);
            } else {
                // æ¸…ç†å“åº”æ–‡æœ¬
                std::string clean_response = client.response;
                
                // ç§»é™¤æ— æ•ˆå­—ç¬¦
                while (!clean_response.empty() && 
                       (clean_response.front() == '?' || 
                        clean_response.front() < 32 || 
                        clean_response.front() > 126)) {
                    clean_response = clean_response.substr(1);
                }
                
                // ç§»é™¤é¦–å°¾ç©ºç™½
                while (!clean_response.empty() && isspace(clean_response.back())) {
                    clean_response.pop_back();
                }
                while (!clean_response.empty() && isspace(clean_response.front())) {
                    clean_response = clean_response.substr(1);
                }
                
                // æˆªæ–­å¯¹è¯ç»ˆæ­¢æ ‡è®°
                const size_t pos = clean_response.find("\n\nUser:");
                if (pos != std::string::npos) {
                    clean_response = clean_response.substr(0, pos);
                }
                
                (*results_out)[i] = string_to_c_str(clean_response);
            }
        }
        
        // ğŸ” è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        #ifdef NEWRLLAMA_DEBUG
        std::cout << "=== Parallel Generation Performance ===" << std::endl;
        std::cout << "Total time: " << total_time << "s" << std::endl;
        std::cout << "Prompt tokens: " << n_total_prompt << std::endl;
        std::cout << "Generated tokens: " << n_total_gen << std::endl;
        std::cout << "Cache misses: " << n_cache_miss << std::endl;
        std::cout << "Prompt speed: " << n_total_prompt / total_time << " t/s" << std::endl;
        std::cout << "Generation speed: " << n_total_gen / total_time << " t/s" << std::endl;
        #endif
        
        return NEWRLLAMA_SUCCESS;
        
    } catch (const std::exception& e) {
        // ğŸš¨ å…¨å±€é”™è¯¯å¤„ç†
        for (const auto& client : clients) {
            if (client.seq_id > 0) {
                llama_kv_self_seq_rm(ctx, client.seq_id, 0, -1);
            }
        }
        
        set_error(error_message, std::string("Parallel generation failed: ") + e.what());
        return NEWRLLAMA_ERROR;
    }
}