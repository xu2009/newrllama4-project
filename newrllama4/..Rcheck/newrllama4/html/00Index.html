<!DOCTYPE html>
<html>
<head><title>R: R Interface to Llama.cpp with Runtime Library Loading and Model
Download</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body><div class="container">
<h1> R Interface to Llama.cpp with Runtime Library Loading and Model
Download
<img class="toplogo" src="../../../doc/html/Rlogo.svg" alt="[R logo]" />
</h1>
<hr/>
<div style="text-align: center;">
<a href="../../../doc/html/packages.html"><img class="arrow" src="../../../doc/html/left.jpg" alt="[Up]" /></a>
<a href="../../../doc/html/index.html"><img class="arrow" src="../../../doc/html/up.jpg" alt="[Top]" /></a>
</div><h2>Documentation for package &lsquo;newrllama4&rsquo; version 1.0.57</h2>

<ul><li><a href="../DESCRIPTION">DESCRIPTION file</a>.</li>
</ul>

<h2>Help Pages</h2>


<table style="width: 100%;">
<tr><td style="width: 25%;"><a href="newrllama4-package.html">newrllama4-package</a></td>
<td>R Interface to llama.cpp with Runtime Library Loading</td></tr>
<tr><td style="width: 25%;"><a href="apply_chat_template.html">apply_chat_template</a></td>
<td>Apply Chat Template to Format Conversations</td></tr>
<tr><td style="width: 25%;"><a href="backend_free.html">backend_free</a></td>
<td>Free newrllama backend</td></tr>
<tr><td style="width: 25%;"><a href="backend_init.html">backend_init</a></td>
<td>Initialize newrllama backend</td></tr>
<tr><td style="width: 25%;"><a href="context_create.html">context_create</a></td>
<td>Create Inference Context for Text Generation</td></tr>
<tr><td style="width: 25%;"><a href="detokenize.html">detokenize</a></td>
<td>Convert Token IDs Back to Text</td></tr>
<tr><td style="width: 25%;"><a href="download_model.html">download_model</a></td>
<td>Download a model manually</td></tr>
<tr><td style="width: 25%;"><a href="generate.html">generate</a></td>
<td>Generate Text Using Language Model Context</td></tr>
<tr><td style="width: 25%;"><a href="generate_parallel.html">generate_parallel</a></td>
<td>Generate text in parallel</td></tr>
<tr><td style="width: 25%;"><a href="get_lib_path.html">get_lib_path</a></td>
<td>Get Backend Library Path</td></tr>
<tr><td style="width: 25%;"><a href="get_model_cache_dir.html">get_model_cache_dir</a></td>
<td>Get the model cache directory</td></tr>
<tr><td style="width: 25%;"><a href="install_newrllama.html">install_newrllama</a></td>
<td>Install newrllama Backend Library</td></tr>
<tr><td style="width: 25%;"><a href="lib_is_installed.html">lib_is_installed</a></td>
<td>Check if Backend Library is Installed</td></tr>
<tr><td style="width: 25%;"><a href="model_load.html">model_load</a></td>
<td>Load Language Model with Automatic Download Support</td></tr>
<tr><td style="width: 25%;"><a href="newrllama4-package.html">newrllama4</a></td>
<td>R Interface to llama.cpp with Runtime Library Loading</td></tr>
<tr><td style="width: 25%;"><a href="quick_llama.html">quick_llama</a></td>
<td>Quick LLaMA Inference</td></tr>
<tr><td style="width: 25%;"><a href="quick_llama_reset.html">quick_llama_reset</a></td>
<td>Reset quick_llama state</td></tr>
<tr><td style="width: 25%;"><a href="tokenize.html">tokenize</a></td>
<td>Convert Text to Token IDs</td></tr>
<tr><td style="width: 25%;"><a href="tokenize_test.html">tokenize_test</a></td>
<td>Test tokenize function (debugging)</td></tr>
</table>
</div></body></html>
