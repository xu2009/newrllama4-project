library(readstata13)
library(cregg)
# load function that does clustered SEs
vcovCluster <- function(
model,
cluster
)
{
require(sandwich)
require(lmtest)
if(nrow(model.matrix(model))!=length(cluster)){
stop("check your data: cluster variable has different N than model")
}
M <- length(unique(cluster))
N <- length(cluster)
K <- model$rank
if(M<50){
warning("Fewer than 50 clusters, variances may be unreliable (could try block bootstrap instead).")
}
dfc <- (M/(M - 1)) * ((N - 1)/(N - K))
uj  <- apply(estfun(model), 2, function(x) tapply(x, cluster, sum));
rcse.cov <- dfc * sandwich(model, meat = crossprod(uj)/N)
return(rcse.cov)
}
# read data
d <- read.dta13("conjoint_ps6.dta")
View(d)
lm <- lm(outcome ~ atgen + atexp + atinti + atmita + atinte + atpol + atven, data=d)
lm_c <- round(coeftest(lm, vcov = vcovCluster(lm, cluster = d$idnum)),4)
lm_c
# unique levels of the attributes
levels(d$atgen)
# unique values of attributes
unique(d$atgen)
# unique values of attributes, such as 0 anf 1
unique(d$atgen)
table(d$atmita)
unique(d$atmita)
levels(d$atmita)
contrasts(d$atmita)
lm_interact <- lm(outcome ~ rural + atgen + atexp + atinti + atmita + atinte + atpol + atven + rural:atgen + rural:atexp + rural:atinti + rural:atmita + rural:atinte + rural:atpol + rural:atven, data=d)
lm_interact_c <- round(coeftest(lm_interact, vcov = vcovCluster(lm_interact, cluster = d$idnum)),4)
lm_interact_c
library(dplyr)
d <- d %>%
mutate(across(c(atgen, atexp, atinti, atmita, atinte, atpol, atven), as.factor))
q3 <- cj(d, outcome ~ atgen + atexp + atinti + atmita + atinte + atpol + atven, id = ~idnum)
head(q3[c("feature", "level", "estimate", "std.error")], 20L)
plot(q3)
d$rural <- as.factor(d$rural)
d_rural <- d[d$rural == 1, ]
d_urban <- d[d$rural == 0, ]
amce_rural <- cj(d_rural, outcome ~ atgen + atexp + atinti + atmita + atinte + atpol + atven, id = ~idnum)
amce_urban <- cj(d_urban, outcome ~ atgen + atexp + atinti + atmita + atinte + atpol + atven, id = ~idnum)
plot(amce_rural, main = "AMCE for Rural Voters")
plot(amce_urban, main = "AMCE for Urban Voters")
d$rural_num <- as.numeric(as.character(d$rural))
d3 = lm(rural_num ~ as.factor(atgen) + as.factor(atexp) + as.factor(atinti) + as.factor(atmita) + as.factor(atinte) + as.factor(atpol) + as.factor(atven), data=d)
d3_cluster = round(coeftest(d3, vcov = vcovCluster(d3, cluster = d$idnum)),2)
d3_cluster
d$fpair = as.factor(d$pair)
d1 = lm(outcome ~ atgen + atexp + atinti + atmita + atinte + atpol + atven +
fpair +
atgen*fpair + atexp*fpair + atinti*fpair + atmita*fpair + atinte*fpair + atpol*fpair + atven*fpair, data=d)
d1_cluster = round(coeftest(d1, vcov = vcovCluster(d1, cluster = d$idnum)),2)
d1_cluster
d$fcandidate = as.factor(d$candidate)
d2 = lm(outcome ~ atgen + atexp + atinti + atmita + atinte + atpol + atven +
fcandidate +
atgen*candidate + atexp*candidate + atinti*candidate + atmita*candidate + atinte*candidate + atpol*candidate + atven*candidate, data=d)
d2_cluster = round(coeftest(d2, vcov = vcovCluster(d2, cluster = d$idnum)),2)
d2_cluster
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("/Users/yaoshengleo/Library/CloudStorage/Dropbox/Purdue University/2025 Spring/POL 605 Causal Inference/PS6")
library(optmatch)
library(foreign)
library(designmatch)
library(stargazer)
library(Hmisc)
library(gridExtra)
library(exactRankTests)
library(ggplot2)
library(reshape2)
library(xtable)
library(sensitivitymw)
library(sensitivitymv)
library(readstata13)
#read data
d <- read.dta13("matching_ps6.dta")
# list all the name of the variables
names(d)
# naive regression
naive_model1 <- lm(outcome_crime_stronghand_wave4 ~ t_ind, data = d)
summary(naive_model1)
# naive regression with covariates
naive_model2 <- lm(outcome_crime_stronghand_wave4 ~ t_ind + education + support_deathpenalty + job_formal_sector + support_democracy + militaryfeelingthermometer + age + white + ideology + support_repression + catholic, data = d)
summary(naive_model2)
View(d)
library(MatchIt)
m.out0 <- matchit(t_ind ~ education + support_deathpenalty + job_formal_sector + support_democracy + militaryfeelingthermometer + age + white + ideology + support_repression + catholic,
data = d,
method = NULL,
distance = "glm")
summary(m.out0)
m.out1 <- matchit(t_ind ~ education + support_deathpenalty + job_formal_sector + support_democracy + militaryfeelingthermometer + age + white + ideology + support_repression + catholic,
data = d,
method = "nearest",
distance = "glm")
summary(m.out1, un=FALSE)
plot(m.out1, type="jitter", interactive= FALSE)
plot(summary(m.out1))
m.out2 <- matchit(t_ind ~ education + support_deathpenalty + job_formal_sector + support_democracy + militaryfeelingthermometer + age + white + ideology + support_repression + catholic,
data = d,
method = "cem",
estimand = "ATT")
summary(m.out2, un=FALSE)
plot(summary(m.out2))
m.out3 <- matchit(t_ind ~ education + support_deathpenalty + job_formal_sector + support_democracy + militaryfeelingthermometer + age + white + ideology + support_repression + catholic,
data = d,
method = "cardinality",
estimand = "ATT",
ratio = 1,
solver = "highs")
summary(m.out3, un=FALSE)
plot(summary(m.out3))
m.data <- match.data(m.out3)
head(m.data)
summary(lm(outcome_crime_stronghand_wave4 ~ t_ind, data = m.data))
m.data2 <- match.data(m.out2)
head(m.data2)
# calculate the difference-in-means by hand
mean_treated <- mean(m.data2$outcome_crime_stronghand_wave4[m.data2$t_ind == 1])
mean_control <- mean(m.data2$outcome_crime_stronghand_wave4[m.data2$t_ind == 0])
mean_diff <- mean_treated - mean_control
mean_diff
test_match3 = data.frame(m.data$outcome_crime_stronghand_wave4[m.data$t_ind==1],m.data$outcome_crime_stronghand_wave4[m.data$t_ind==0])
colnames(test_match3) = c("treated","control")
# Gamma = 1
senmw(test_match3,gamma=1,method="t")$pval
# Gamma = 1.1
senmw(test_match3,gamma=1.1,method="t")$pval
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library(newrllama)
init_llama(
model_path   = "/Users/yaoshengleo/Desktop/gguf模型/gemma-2-2b-it-Q5_K_M.gguf",
n_gpu_layers = 50,     # ↓ if VRAM is tight
n_ctx        = 2048,   # tokens per sequence
n_threads    = 8       # CPU threads
)
txt <- llama_infer(
prompt         = "Introduce West Lafayette.",
max_tokens     = 50,
top_k          = 20,
top_p          = 0.9,
temperature    = 0,        # 0 = deterministic / greedy
repeat_last_n  = 128,
penalty_repeat = 1.15,
seed           = 12345
)
cat(txt)
cat(llama_chat("Where is Purdue University?",max_tokens = 10), "\n")
cat(llama_chat("Who is the current president of the university?",max_tokens = 10), "\n")
llama_chat_reset()   # wipe history when you’re done
qs <- c("Explain entropy.",
"888 + 890 = ?",
"What is ggplot2?")
ans <- llama_batch(qs,
max_tokens     = 40,
repeat_last_n  = 128,
penalty_repeat = 1.15)
print(ans)
llama_sentiment("I missed my flight. Terrible day!")
#> [1] Negative
# 设置参数
set.seed(123) # 设置随机数种子，确保每次运行代码时生成的随机数相同，从而使结果可重复。
alpha <- 0.05  # 定义显著性水平（\alpha），通常设为 0.05，表示在 5% 的错误率下检验显著性。
effect_sizes <- c(0, 0, 0.14, 0.14, 0.14)  # 控制组均值 = 0，处理组均值为小效应（d = 0.14 转换自 f = 0.1）d=f*根号2; d=(x1_mean - x2_mean)/两组合并标准差=MDE/两组合并标准差
sample_sizes <- seq(100, 4000, 100)  # 定义总样本量的范围，从 100 到 1000，每次递增 100。 f=效应的标准差/误差的标准差
simulations <- 1000  # 每种样本量的模拟次数
group_count <- 5  # 实验组数目
# 功效计算函数
simulate_power <- function(sample_size, effect_sizes, alpha, simulations) { # 定义一个函数 simulate_power 来计算某个总样本量下的功效（power）。
significant_results <- 0 # 记录模拟中显著结果的次数，初始化为 0。
# 计算每组样本量
group_size <- sample_size / group_count
cat("Total Sample size:", sample_size,
"Each group size:", group_size, "\n") # cat() 输出总样本量和每组样本量，方便检查分配情况。
for (i in 1:simulations) {
# 模拟每组数据
placebo     <- rnorm(group_size, mean = effect_sizes[1], sd = 1) # 使用 rnorm() 生成每组的随机数据，假设服从正态分布
control     <- rnorm(group_size, mean = effect_sizes[2], sd = 1)
treatment1   <- rnorm(group_size, mean = effect_sizes[3], sd = 1) # mean：每组的均值，取自 effect_sizes。
treatment2   <- rnorm(group_size, mean = effect_sizes[4], sd = 1) # sd = 1：标准差为 1，表示响应变量的自然波动。
treatment3   <- rnorm(group_size, mean = effect_sizes[5], sd = 1)
# 合并数据
data <- data.frame( # 创建一个数据框 data，包含两列.
group = factor(rep(c("placebo", "Control", "Treatment1",
"Treatment2", "Treatment3"), each = group_size)), # group：表示组别（“Control”, “Low”, “Mid”, “High”）。
trust = c(placebo, control, treatment1, treatment2, treatment3) # trust：模拟的信任得分。
)
# 运行单因素方差分析（ANOVA）
anova_result <- aov(trust ~ group, data = data) # 使用 aov() 检验四组的均值是否存在显著差异。
p_value <- summary(anova_result)[[1]][["Pr(>F)"]][1] # 提取方差分析结果中的 p-值，判断是否拒绝零假设。
# 统计显著性结果
if (p_value < alpha) { # 如果 p-值小于显著性水平  \alpha = 0.05 ，说明组间差异显著。
significant_results <- significant_results + 1 # 记录每次模拟中检测到显著结果的次数。
}
}
# 返回功效
return(significant_results / simulations) # 返回功效值，即显著结果占总模拟次数的比例。例如，如果 1000 次模拟中有 800 次检测到显著差异，则功效为 0.8（80%）。
}
# 计算不同样本量下的功效
power_results <- sapply(sample_sizes, simulate_power, effect_sizes, alpha, simulations) # 使用 sapply() 对每个样本量运行 simulate_power 函数。生成每个样本量对应的功效。
# 可视化功效
plot(sample_sizes, power_results, type = "b", ylim = c(0, 1),
xlab = "Sample Size", ylab = "Power",
main = "Power to Detect Effect on Trust by Sample Size with 0.1 Standardized Effect")
abline(h = 0.8, col = "red", lty = 2)  # 标记 80% 功效线
# 6. 加载包
library(newrllama)
# 在你的 R 脚本中
# library(yourLlamaPackageName) # 确保你的包已加载
cat("R_SCRIPT: Calling llama_backend_init_wrapper()...\n")
llama_backend_init_wrapper()
cat("R_SCRIPT: llama_backend_init_wrapper() finished.\n")
cat("R_SCRIPT: Calling llama_load_model()...\n")
# 你的模型加载代码，例如:
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Llama-3.2-1B-Instruct.Q8_0.gguf"
model_ptr <- tryCatch({
llama_load_model(model_path, n_gpu_layers = 20, use_mmap = TRUE, use_mlock = FALSE)
}, error = function(e) {
cat("R_SCRIPT: Error during llama_load_model():\n")
print(e)
NULL
})
if (is.null(model_ptr)) {
cat("R_SCRIPT: model_ptr is NULL after llama_load_model().\n")
# 这里会触发你之前的错误信息
stop("所有尝试均未能成功加载模型。请检查模型路径、显存和错误信息。")
} else {
cat("R_SCRIPT: model_ptr seems to be loaded.\n")
print(model_ptr)
}
ctx_ptr_single <- llama_create_context(model_ptr, n_ctx = 2048, n_threads = 8, n_seq_max = 1)
# 确认实际导出的函数名
cat("newrllama 载入完成，当前可用函数: \n")
print(ls("package:newrllama")) # 确认 llama_load_model 和 llama_backend_init_wrapper 存在
# 4. 创建单序列上下文
print(paste("Current R working directory (getwd()):", getwd()))
# 这个是 ggml-metal.m 回退时尝试的目录路径 (根据你的日志)
expected_shader_dir <- "/Users/yaoshengleo/Desktop/newrllama_2/"
print(paste("Directory ggml-metal is checking for shaders (CWD fallback):", expected_shader_dir))
# 检查 ggml-metal.metal 和 ggml-common.h 是否真的在这个目录中
metal_file_path <- file.path(expected_shader_dir, "ggml-metal.metal")
common_header_path <- file.path(expected_shader_dir, "ggml-common.h")
print(paste("Does 'ggml-metal.metal' exist at expected_shader_dir?", file.exists(metal_file_path)))
print(paste("Does 'ggml-common.h' exist at expected_shader_dir?", file.exists(common_header_path)))
print(paste("Listing all files in '", expected_shader_dir, "':"))
print(list.files(expected_shader_dir))
# (然后是你调用 llama_create_context 的代码)
# ctx_ptr_single <- llama_create_context(...)
# 假设 ggml-metal.metal 和 ggml-common.h 都已确认在下面这个目录
metal_resources_path <- "/Users/yaoshengleo/Desktop/newrllama_2" # 确保这个路径是正确的
Sys.setenv(GGML_METAL_PATH_RESOURCES = metal_resources_path)
print(paste("GGML_METAL_PATH_RESOURCES set to:", Sys.getenv("GGML_METAL_PATH_RESOURCES")))
# (然后是你调用 llama_create_context 的代码)
# ctx_ptr_single <- llama_create_context(...)
ctx_ptr_single <- llama_create_context(model_ptr, n_ctx = 2048, n_threads = 8, n_seq_max = 1)
cat("单序列上下文创建成功\n")
## ------------------ 第五部分：并行多序列生成 ------------------- ##
cat("\n=== 第五部分：并行序列生成 ===\n")
ctx_ptr_parallel <- llama_create_context(model_ptr, n_ctx = 2048, n_threads = 8, n_seq_max = 3)
prompts_raw <- c(
"Please introduce Purdue University in West Lafayette, Indiana.",
"What is the answer of five plus three?",
"What is sentiment of the following text: 'I love programming in R!'?"
)
formatted_prompts <- vapply(prompts_raw, function(p) {
llama_apply_chat_template_wrapper(model_ptr, "llama3", list(
list(role = "system", content = "You are a helpful assistant."),
list(role = "user",   content = p)
), TRUE)
}, character(1))
parallel_out <- llama_generate_parallel(
ctx_ptr_parallel, formatted_prompts,
max_tokens     = 20,
top_k          = 30,
top_p          = 0.9,
temperature    = 0.6,
repeat_last_n  = 64,
penalty_repeat = 1.15,
seed           = 54321
)
cat("并行生成完成：\n--------------------------\n")
for (i in seq_along(prompts_raw)) {
cat("提示 ", i, "：", prompts_raw[i], "\n", sep = "")
cat("回答 ", i, "：", parallel_out[[i]], "\n--------------------------\n", sep = "")
}
llama_free_context_wrapper(ctx_ptr_single)
llama_free_context_wrapper(ctx_ptr_parallel)
llama_free_model_wrapper(model_ptr)
llama_backend_free_wrapper()
## ------------------ 第八部分：资源释放 & 结束 ------------------ ##
cat("\n=== 第八部分：资源释放和清理 ===\n")
llama_free_context_wrapper(ctx_ptr_single)
llama_free_context_wrapper(ctx_ptr_parallel)
llama_free_model_wrapper(model_ptr)
llama_backend_free_wrapper()
cat("资源已全部释放，测试流程完成！\n")
llama_free_context_wrapper(ctx_ptr_parallel)
# 6. 加载包
library(newrllama)
# 在你的 R 脚本中
# library(yourLlamaPackageName) # 确保你的包已加载
cat("R_SCRIPT: Calling llama_backend_init_wrapper()...\n")
llama_backend_init_wrapper()
cat("R_SCRIPT: llama_backend_init_wrapper() finished.\n")
cat("R_SCRIPT: Calling llama_load_model()...\n")
# 你的模型加载代码，例如:
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Llama-3.2-1B-Instruct.Q8_0.gguf"
model_ptr <- tryCatch({
llama_load_model(model_path, n_gpu_layers = 20, use_mmap = TRUE, use_mlock = FALSE)
}, error = function(e) {
cat("R_SCRIPT: Error during llama_load_model():\n")
print(e)
NULL
})
if (is.null(model_ptr)) {
cat("R_SCRIPT: model_ptr is NULL after llama_load_model().\n")
# 这里会触发你之前的错误信息
stop("所有尝试均未能成功加载模型。请检查模型路径、显存和错误信息。")
} else {
cat("R_SCRIPT: model_ptr seems to be loaded.\n")
print(model_ptr)
}
ctx_ptr_single <- llama_create_context(model_ptr, n_ctx = 2048, n_threads = 8, n_seq_max = 1)
# 确认实际导出的函数名
cat("newrllama 载入完成，当前可用函数: \n")
print(ls("package:newrllama")) # 确认 llama_load_model 和 llama_backend_init_wrapper 存在
# 4. 创建单序列上下文
print(paste("Current R working directory (getwd()):", getwd()))
# 这个是 ggml-metal.m 回退时尝试的目录路径 (根据你的日志)
expected_shader_dir <- "/Users/yaoshengleo/Desktop/newrllama_2/"
print(paste("Directory ggml-metal is checking for shaders (CWD fallback):", expected_shader_dir))
# 检查 ggml-metal.metal 和 ggml-common.h 是否真的在这个目录中
metal_file_path <- file.path(expected_shader_dir, "ggml-metal.metal")
common_header_path <- file.path(expected_shader_dir, "ggml-common.h")
print(paste("Does 'ggml-metal.metal' exist at expected_shader_dir?", file.exists(metal_file_path)))
print(paste("Does 'ggml-common.h' exist at expected_shader_dir?", file.exists(common_header_path)))
print(paste("Listing all files in '", expected_shader_dir, "':"))
print(list.files(expected_shader_dir))
# (然后是你调用 llama_create_context 的代码)
# ctx_ptr_single <- llama_create_context(...)
# 假设 ggml-metal.metal 和 ggml-common.h 都已确认在下面这个目录
metal_resources_path <- "/Users/yaoshengleo/Desktop/newrllama_2" # 确保这个路径是正确的
Sys.setenv(GGML_METAL_PATH_RESOURCES = metal_resources_path)
print(paste("GGML_METAL_PATH_RESOURCES set to:", Sys.getenv("GGML_METAL_PATH_RESOURCES")))
# (然后是你调用 llama_create_context 的代码)
# ctx_ptr_single <- llama_create_context(...)
ctx_ptr_single <- llama_create_context(model_ptr, n_ctx = 2048, n_threads = 8, n_seq_max = 1)
cat("单序列上下文创建成功\n")
## ---------------- 第三部分：模板 / 分词 / 去分词 --------------- ##
cat("\n=== 第三部分：模板应用和分词功能 ===\n")
chat_messages <- list(
list(role = "system", content = "You are a helpful assistant."),
list(role = "user",   content = "Please introduce the city of West Lafayette, Indiana.")
)
# 1. 应用 llama3 模板
formatted_prompt <- llama_apply_chat_template_wrapper(model_ptr, "llama3", chat_messages, add_ass = TRUE)
cat("格式化提示 (首 200 字符)：\n", substr(formatted_prompt, 1, 200), "…\n\n")
# 2. 分词 & 去分词校验
tokens <- llama_tokenize(model_ptr, formatted_prompt, add_bos = TRUE)
cat("Token 数：", length(tokens), "，前 10 个：", paste(head(tokens, 10), collapse = ", "), "\n")
detok <- llama_detokenize(model_ptr, tokens)
cat("去分词校验 (首 120 字符)：\n", substr(detok, 1, 120), "…\n\n")
## ------------------ 第四部分：单序列文本生成 -------------------- ##
cat("\n=== 第四部分：单序列文本生成 ===\n")
gen_text <- llama_generate(
ctx_ptr_single, tokens,
max_tokens     = 50,
top_k          = 20,
top_p          = 0.9,
temperature    = 0.7,
repeat_last_n  = 64,
penalty_repeat = 1.1,
seed           = 12345
)
cat("生成结果:\n================\n", gen_text, "\n================\n\n", sep = "")
## ---------------- 第三部分：模板 / 分词 / 去分词 --------------- ##
cat("\n=== 第三部分：模板应用和分词功能 ===\n")
chat_messages <- list(
list(role = "system", content = "You are a helpful assistant."),
list(role = "user",   content = "what is the answer of 5 plus 3?")
)
# 1. 应用 llama3 模板
formatted_prompt <- llama_apply_chat_template_wrapper(model_ptr, "llama3", chat_messages, add_ass = TRUE)
cat("格式化提示 (首 200 字符)：\n", substr(formatted_prompt, 1, 200), "…\n\n")
# 2. 分词 & 去分词校验
tokens <- llama_tokenize(model_ptr, formatted_prompt, add_bos = TRUE)
cat("Token 数：", length(tokens), "，前 10 个：", paste(head(tokens, 10), collapse = ", "), "\n")
detok <- llama_detokenize(model_ptr, tokens)
cat("去分词校验 (首 120 字符)：\n", substr(detok, 1, 120), "…\n\n")
## ------------------ 第四部分：单序列文本生成 -------------------- ##
cat("\n=== 第四部分：单序列文本生成 ===\n")
gen_text <- llama_generate(
ctx_ptr_single, tokens,
max_tokens     = 100,
top_k          = 20,
top_p          = 0.9,
temperature    = 0.7,
repeat_last_n  = 64,
penalty_repeat = 1.1,
seed           = 12345
)
cat("生成结果:\n================\n", gen_text, "\n================\n\n", sep = "")
cat("第二轮回答：\n================\n", ans_r2, "\n================\n\n", sep = "")
cat("\n=== 第八部分：资源释放和清理 ===\n")
llama_free_context_wrapper(ctx_ptr_single)
llama_free_context_wrapper(ctx_ptr_parallel)
llama_free_context_wrapper(ctx_ptr_single)
## ------------------ 第五部分：并行多序列生成 ------------------- ##
cat("\n=== 第五部分：并行序列生成 ===\n")
ctx_ptr_parallel <- llama_create_context(model_ptr, n_ctx = 2048, n_threads = 8, n_seq_max = 3)
prompts_raw <- c(
"Please introduce Purdue University in West Lafayette, Indiana.",
"What is the answer of five plus three?",
"What is sentiment of the following text: 'I love programming in R!'?"
)
formatted_prompts <- vapply(prompts_raw, function(p) {
llama_apply_chat_template_wrapper(model_ptr, "llama3", list(
list(role = "system", content = "You are a helpful assistant."),
list(role = "user",   content = p)
), TRUE)
}, character(1))
parallel_out <- llama_generate_parallel(
ctx_ptr_parallel, formatted_prompts,
max_tokens     = 20,
top_k          = 30,
top_p          = 0.9,
temperature    = 0.6,
repeat_last_n  = 64,
penalty_repeat = 1.15,
seed           = 54321
)
cat("并行生成完成：\n--------------------------\n")
for (i in seq_along(prompts_raw)) {
cat("提示 ", i, "：", prompts_raw[i], "\n", sep = "")
cat("回答 ", i, "：", parallel_out[[i]], "\n--------------------------\n", sep = "")
}
# 1. 加载包
cat("📦 [1/5] 加载 newrllama4 包...\n")
library(newrllama4)
# 2. 检查并安装后端库
cat("⬇️  [2/5] 检查预编译后端库...\n")
if (!lib_is_installed()) {
cat("    正在下载预编译库...\n")
install_newrllama()
} else {
cat("    ✅ 后端库已安装\n")
}
# 3. 初始化后端
cat("🔧 [3/5] 初始化后端...\n")
backend_init()
# 4. 加载模型
cat("📚 [4/5] 加载 Llama 模型...\n")
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("❌ 请更新模型路径\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    ✅ 模型加载成功 (Metal GPU 加速)\n")
# 创建推理上下文
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# 单序列生成
cat("═══ 单序列文本生成 ═══\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("输入: %s\n", prompt))
cat(sprintf("输出: %s\n\n", result))
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s → %s\n", i, prompts[i], results[i]))
}
cat("\n🎉 演示完成！所有功能正常工作\n")
# 清理
backend_free()
# 1. 安装或加载devtools
# install.packages("devtools")
library(devtools)
# 2. 设置工作目录到您的R包内部
setwd("~/Desktop/newrllama_4_project/newrllama4")
# 3. 运行检查
#    这个函数会自动处理构建和检查的全过程
#    args = "--as-cran" 确保了最严格的检查
check(args = "--as-cran")
