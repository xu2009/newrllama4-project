% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/quick_llama.R
\name{quick_llama}
\alias{quick_llama}
\title{Quick LLaMA Inference}
\usage{
quick_llama(
  prompt,
  model = .get_default_model(),
  n_threads = NULL,
  n_gpu_layers = "auto",
  n_ctx = 2048L,
  max_tokens = 100L,
  temperature = 0.7,
  top_p = 0.9,
  top_k = 40L,
  repeat_penalty = 1.1,
  min_p = 0.05,
  stream = NULL,
  seed = 1234L,
  ...
)
}
\arguments{
\item{prompt}{Character string or vector of prompts to process}

\item{model}{Model URL or path (default: Llama-3.2-1B-Instruct Q4_K_M)}

\item{n_threads}{Number of threads (default: auto-detect)}

\item{n_gpu_layers}{Number of GPU layers (default: auto-detect)}

\item{n_ctx}{Context size (default: 2048)}

\item{max_tokens}{Maximum tokens to generate (default: 100)}

\item{temperature}{Sampling temperature (default: 0.7)}

\item{top_p}{Top-p sampling (default: 0.9)}

\item{top_k}{Top-k sampling (default: 40)}

\item{repeat_penalty}{Repetition penalty (default: 1.1)}

\item{min_p}{Minimum probability threshold (default: 0.05)}

\item{stream}{Whether to stream output (default: auto-detect based on interactive())}

\item{seed}{Random seed for reproducibility (default: 1234)}

\item{...}{Additional parameters passed to generate() or generate_parallel()}
}
\value{
Character string (single prompt) or named list (multiple prompts)
}
\description{
A high-level convenience function that provides one-line LLM inference.
Automatically handles model downloading, loading, and text generation.
}
\examples{
\dontrun{
# Simple usage
response <- quick_llama("Hello, how are you?")

# Multiple prompts
responses <- quick_llama(c("Summarize AI", "Explain quantum computing"))

# Custom parameters
creative_response <- quick_llama("Tell me a story", 
                                 temperature = 0.9, 
                                 max_tokens = 200)
}
}
