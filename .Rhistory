library(newrllama4)
# 2. 检查并安装后端库
cat("⬇️  [2/5] 检查预编译后端库...\n")
if (!lib_is_installed()) {
cat("    正在下载预编译库...\n")
install_newrllama()
} else {
cat("    ✅ 后端库已安装\n")
}
# 3. 初始化后端
cat("🔧 [3/5] 初始化后端...\n")
backend_init()
# 4. 加载模型
cat("📚 [4/5] 加载 Llama 模型...\n")
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("❌ 请更新模型路径\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    ✅ 模型加载成功 (Metal GPU 加速)\n")
# 创建推理上下文
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# 单序列生成
cat("═══ 单序列文本生成 ═══\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("输入: %s\n", prompt))
cat(sprintf("输出: %s\n\n", result))
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
)
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting.",
)
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s → %s\n", i, prompts[i], results[i]))
}
cat("\n🎉 演示完成！所有功能正常工作\n")
# 清理
backend_free()
gc()
#!/usr/bin/env Rscript
# =============================================================================
# newrllama4 Package Demo - 完整功能展示
# =============================================================================
cat("🚀 newrllama4 Package Demo\n")
cat("展示完整的LLM推理流程：库加载 → 后端安装 → 模型加载 → 文本生成\n\n")
# 1. 加载包
cat("📦 [1/5] 加载 newrllama4 包...\n")
library(newrllama4)
# 2. 检查并安装后端库
cat("⬇️  [2/5] 检查预编译后端库...\n")
if (!lib_is_installed()) {
cat("    正在下载预编译库...\n")
install_newrllama()
} else {
cat("    ✅ 后端库已安装\n")
}
# 3. 初始化后端
cat("🔧 [3/5] 初始化后端...\n")
backend_init()
# 4. 加载模型
cat("📚 [4/5] 加载 Llama 模型...\n")
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("❌ 请更新模型路径\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    ✅ 模型加载成功 (Metal GPU 加速)\n")
# 创建推理上下文
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# 单序列生成
cat("═══ 单序列文本生成 ═══\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("输入: %s\n", prompt))
cat(sprintf("输出: %s\n\n", result))
# 并行序列生成
cat("═══ 并行序列文本生成 ═══\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s → %s\n", i, prompts[i], results[i]))
}
cat("\n🎉 演示完成！所有功能正常工作\n")
# 清理
backend_free()
library(newrllama4)
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/gemma-3-12b-it-Q8_0.gguf"
# 首先检查内存和加载模型
cat("\n=== 内存检查和模型加载 ===\n")
tryCatch({
# 估算内存需求
estimated_memory <- .Call("c_r_estimate_model_memory", model_path)
cat("估算内存需求:", round(estimated_memory / (1024^3), 2), "GB\n")
# 检查可用内存
memory_available <- .Call("c_r_check_memory_available", estimated_memory)
cat("内存是否足够:", memory_available, "\n")
if (!memory_available) {
stop("内存不足，无法加载12B模型")
}
# 加载模型
cat("\n正在加载12B模型...\n")
model <- model_load(model_path,
check_memory = TRUE,
verify_integrity = TRUE,
n_gpu_layers = 0L,
use_mmap = TRUE)
cat("✓ 12B模型加载成功！\n")
# 创建上下文
cat("\n=== 创建上下文 ===\n")
context <- context_create(model, n_ctx = 1024, n_threads = 4)
cat("✓ 上下文创建成功\n")
# 测试1: 单序列生成
cat("\n=== 测试1: 单序列文本生成 ===\n")
test_prompts <- c(
"What is artificial intelligence?",
"Explain quantum computing in simple terms.",
"Write a short poem about nature."
)
# 首先检查内存和加载模型
cat("\n=== 内存检查和模型加载 ===\n")
tryCatch({
# 估算内存需求
estimated_memory <- .Call("c_r_estimate_model_memory", model_path)
cat("估算内存需求:", round(estimated_memory / (1024^3), 2), "GB\n")
# 检查可用内存
memory_available <- .Call("c_r_check_memory_available", estimated_memory)
cat("内存是否足够:", memory_available, "\n")
if (!memory_available) {
stop("内存不足，无法加载12B模型")
}
# 加载模型
cat("\n正在加载12B模型...\n")
model <- model_load(model_path,
check_memory = TRUE,
verify_integrity = TRUE,
n_gpu_layers = 0L,
use_mmap = TRUE)
cat("✓ 12B模型加载成功！\n")
# 创建上下文
cat("\n=== 创建上下文 ===\n")
context <- context_create(model, n_ctx = 1024, n_threads = 4)
cat("✓ 上下文创建成功\n")
# 测试1: 单序列生成
cat("\n=== 测试1: 单序列文本生成 ===\n")
test_prompts <- c(
"What is artificial intelligence?",
"Explain quantum computing in simple terms.",
"Write a short poem about nature."
)
for (i in seq_along(test_prompts)) {
cat("\n--- 测试", i, "---\n")
cat("输入:", test_prompts[i], "\n")
start_time <- Sys.time()
tryCatch({
tokens <- tokenize(model, test_prompts[i], add_special = TRUE)
result <- generate(context, tokens, max_tokens = 50)
end_time <- Sys.time()
cat("输出:", result, "\n")
cat("生成时间:", round(as.numeric(end_time - start_time), 2), "秒\n")
cat("✓ 单序列生成成功\n")
}, error = function(e) {
cat("✗ 单序列生成失败:", e$message, "\n")
})
}
# 测试2: 并行生成（如果函数存在）
cat("\n=== 测试2: 并行文本生成 ===\n")
# 检查是否有并行生成函数
parallel_functions <- c("generate_parallel", "parallel_generate")
parallel_func <- NULL
for (func_name in parallel_functions) {
if (exists(func_name, mode = "function")) {
parallel_func <- get(func_name)
cat("找到并行函数:", func_name, "\n")
break
}
}
if (!is.null(parallel_func)) {
# 准备并行测试数据
parallel_prompts <- c(
"Tell me about machine learning.",
"What is the future of technology?",
"Describe the solar system."
)
cat("开始并行生成测试...\n")
start_time <- Sys.time()
tryCatch({
# 尝试并行生成
parallel_results <- parallel_func(
model = model,
prompts = parallel_prompts,
max_tokens = 30,
n_threads = 4
)
end_time <- Sys.time()
parallel_time <- as.numeric(end_time - start_time)
cat("✓ 并行生成成功！\n")
cat("并行生成时间:", round(parallel_time, 2), "秒\n")
cat("平均每个序列:", round(parallel_time / length(parallel_prompts), 2), "秒\n")
# 显示并行结果
for (i in seq_along(parallel_prompts)) {
cat("\n--- 并行结果", i, "---\n")
cat("输入:", parallel_prompts[i], "\n")
cat("输出:", parallel_results[i], "\n")
}
}, error = function(e) {
cat("✗ 并行生成失败:", e$message, "\n")
})
} else {
cat("未找到并行生成函数，跳过并行测试\n")
cat("可用的生成函数:\n")
funcs <- ls(envir = .GlobalEnv)
generate_funcs <- funcs[grepl("generate", funcs, ignore.case = TRUE)]
if (length(generate_funcs) > 0) {
cat(paste(generate_funcs, collapse = ", "), "\n")
} else {
cat("无其他生成函数\n")
}
}
# 性能总结
cat("\n=== 性能总结 ===\n")
cat("✓ 12B模型成功加载和运行\n")
cat("✓ 单序列生成功能正常\n")
if (!is.null(parallel_func)) {
cat("✓ 并行生成功能测试完成\n")
} else {
cat("- 并行生成功能未找到\n")
}
}, error = function(e) {
cat("\n✗ 测试失败:", e$message, "\n")
# 分析失败原因
if (grepl("内存|memory|Memory", e$message)) {
cat("→ 内存不足导致的失败\n")
cat("→ 内存保护机制正常工作\n")
} else if (grepl("model|模型", e$message)) {
cat("→ 模型加载相关错误\n")
} else {
cat("→ 其他错误类型\n")
}
})
model <- model_load(model_path,
check_memory = TRUE,
verify_integrity = TRUE,
n_gpu_layers = 0L,
use_mmap = TRUE)
library(newrllama4)
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/gemma-3-12b-it-Q8_0.gguf"
model1 <- model_load(model_path,
check_memory = FALSE,  # 禁用内存检查
verify_integrity = FALSE,  # 禁用完整性检查
n_gpu_layers = 0L,
use_mmap = TRUE)
context <- context_create(model1, n_ctx = 512, n_threads = 2)
text <- "Hello"
tokens <- tokenize(model1, text, add_special = TRUE)
cat("✓ tokenize成功，tokens:", length(tokens), "\n")
result <- generate(context, tokens, max_tokens = 10)
cat("✓ 文本生成成功\n")
cat("输入:", text, "\n")
cat("输出:", result, "\n")
# 测试基本功能
cat("\n=== 测试基本功能 ===\n")
context <- context_create(model1, n_ctx = 512, n_threads = 2)
cat("✓ 上下文创建成功\n")
text <- "Hello"
tokens <- tokenize(model1, text, add_special = TRUE)
cat("✓ tokenize成功，tokens:", length(tokens), "\n")
result <- generate(context, tokens, max_tokens = 10)
cat("✓ 文本生成成功\n")
cat("输入:", text, "\n")
cat("输出:", result, "\n")
}, error = function(e) {
#!/usr/bin/env Rscript
# =============================================================================
# 并行生成函数改进测试 - v1.0.55
# =============================================================================
cat("🚀 Testing improved parallel generation - v1.0.55\n")
cat("验证统一批次处理和严格序列隔离的新实现\n\n")
# 1. 加载包
cat("📦 [1/4] 加载 newrllama4 包...\n")
library(newrllama4)
# 2. 检查并安装后端库
cat("⬇️  [2/4] 检查预编译后端库...\n")
if (!lib_is_installed()) {
cat("    正在下载预编译库...\n")
install_newrllama()
} else {
cat("    ✅ 后端库已安装\n")
}
# 3. 初始化后端
cat("🔧 [3/4] 初始化后端...\n")
backend_init()
# 4. 加载模型
cat("📚 [4/4] 加载 Llama 模型...\n")
model_path <- "/Users/yaoshengleo/Desktop/gguf模型/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf"
if (!file.exists(model_path)) {
cat("❌ 请更新模型路径\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    ✅ 模型加载成功 (Metal GPU 加速)\n")
# 创建并行推理上下文
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 8L)
# =============================================================================
# 测试3: 大规模并行处理
# =============================================================================
cat("\n═══ 测试3: 大规模并行处理 ═══\n")
prompts_large <- c(
"Explain photosynthesis.",
"What is the theory of relativity?",
"Describe the water cycle.",
"What is DNA?",
"Explain gravity.",
"What is the solar system?",
"Describe evolution.",
"What is the periodic table?"
)
cat("输入 8 个科学问题进行大规模并行处理...\n")
start_time <- Sys.time()
results_large <- generate_parallel(
context_parallel,
prompts_large,
max_tokens = 20,
temperature = 0.5
)
end_time <- Sys.time()
large_processing_time <- as.numeric(end_time - start_time)
cat("✅ 大规模并行处理完成\n")
cat("📊 处理时间:", round(large_processing_time, 2), "秒\n")
cat("📊 平均每个问题:", round(large_processing_time / length(prompts_large), 2), "秒\n")
success_count <- 0
for (i in seq_along(prompts_large)) {
result <- results_large[i]
if (!is.null(result) && nchar(result) > 0 && !grepl("\\[ERROR\\]", result)) {
success_count <- success_count + 1
}
}
cat(sprintf("📊 成功处理率: %d/%d (%.1f%%)\n",
success_count, length(prompts_large),
success_count / length(prompts_large) * 100))
# =============================================================================
# 测试4: 错误恢复能力
# =============================================================================
cat("\n═══ 测试4: 错误恢复能力 ═══\n")
prompts_mixed <- c(
"What is the capital of France?",
paste(rep("Very long", 100), collapse = " "),  # 可能超长的提示符
"What is 2+2?",
"Tell me a joke."
)
cat("输入包含可能问题的混合提示符...\n")
results_mixed <- generate_parallel(
context_parallel,
prompts_mixed,
max_tokens = 15L,
temperature = 0.7
)
cat("✅ 错误恢复测试完成\n")
for (i in seq_along(prompts_mixed)) {
result <- results_mixed[i]
if (grepl("\\[ERROR\\]", result)) {
cat(sprintf("🔹 问题%d: 出现错误 - %s\n", i, result))
} else {
cat(sprintf("🔹 问题%d: 正常处理 - %s\n", i, substr(result, 1, 50)))
}
}
# 清理
cat("\n🧹 清理资源...\n")
backend_free()
gc()
cat("🎉 v1.0.55 改进版并行生成测试完成！\n")
library(newrllama4)
install_newrllama4()
install_newrllama()
response <- quick_llama("Hello, how are you?")
prompts <- c("Summarize machine learning",
"Explain quantum computing",
"What is artificial intelligence?")
responses <- quick_llama(prompts)
print(responses)
