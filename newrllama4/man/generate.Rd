% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api.R
\name{generate}
\alias{generate}
\title{Generate Text Using Language Model Context}
\usage{
generate(
  context,
  tokens,
  max_tokens = 100L,
  top_k = 20L,
  top_p = 0.9,
  temperature = 0.7,
  repeat_last_n = 64L,
  penalty_repeat = 1.1,
  seed = 1234L
)
}
\arguments{
\item{context}{A context object created with \code{\link{context_create}}}

\item{tokens}{Integer vector of input tokens. Use \code{\link{tokenize}} to convert text to tokens}

\item{max_tokens}{Maximum number of tokens to generate (default: 100). Higher values produce longer responses}

\item{top_k}{Top-k sampling parameter (default: 40). Limits vocabulary to k most likely tokens. Use 0 to disable}

\item{top_p}{Top-p (nucleus) sampling parameter (default: 0.9). Cumulative probability threshold for token selection}

\item{temperature}{Sampling temperature (default: 0.8). Higher values (>1) increase randomness, lower values (<1) make output more deterministic}

\item{repeat_last_n}{Number of recent tokens to consider for repetition penalty (default: 64)}

\item{penalty_repeat}{Repetition penalty strength (default: 1.1). Values >1 discourage repetition, <1 encourage it}

\item{seed}{Random seed for reproducible generation (default: -1 for random). Use positive integers for deterministic output}
}
\value{
Character string containing the generated text
}
\description{
Generates text using a loaded language model context. This is a low-level function
that requires pre-tokenized input. For easier text generation from plain text,
consider using \code{\link{quick_llama}} instead.
}
\examples{
\dontrun{
# Load model and create context
model <- model_load("path/to/model.gguf")
ctx <- context_create(model, n_ctx = 2048)

# Tokenize input text
tokens <- tokenize(model, "Hello, how are you?")

# Generate response
response <- generate(ctx, tokens, max_tokens = 50, temperature = 0.7)

# Creative writing with higher temperature
creative_tokens <- tokenize(model, "Once upon a time")
story <- generate(ctx, creative_tokens, max_tokens = 200, temperature = 1.2)

# Deterministic generation with seed
predictable <- generate(ctx, tokens, max_tokens = 30, temperature = 0.5, seed = 42)
}
}
\seealso{
\code{\link{quick_llama}}, \code{\link{generate_parallel}}, \code{\link{tokenize}}, \code{\link{context_create}}
}
