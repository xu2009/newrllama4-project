library(newrllama4)
# 2. æ£€æŸ¥å¹¶å®‰è£…åç«¯åº“
cat("â¬‡ï¸  [2/5] æ£€æŸ¥é¢„ç¼–è¯‘åç«¯åº“...\n")
if (!lib_is_installed()) {
cat("    æ­£åœ¨ä¸‹è½½é¢„ç¼–è¯‘åº“...\n")
install_newrllama()
} else {
cat("    âœ… åç«¯åº“å·²å®‰è£…\n")
}
# 3. åˆå§‹åŒ–åç«¯
cat("ğŸ”§ [3/5] åˆå§‹åŒ–åç«¯...\n")
backend_init()
# 4. åŠ è½½æ¨¡å‹
cat("ğŸ“š [4/5] åŠ è½½ Llama æ¨¡å‹...\n")
model_path <- "/Users/yaoshengleo/Desktop/ggufæ¨¡å‹/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("âŒ è¯·æ›´æ–°æ¨¡å‹è·¯å¾„\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Metal GPU åŠ é€Ÿ)\n")
# åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# å•åºåˆ—ç”Ÿæˆ
cat("â•â•â• å•åºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("è¾“å…¥: %s\n", prompt))
cat(sprintf("è¾“å‡º: %s\n\n", result))
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
)
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting.",
)
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s â†’ %s\n", i, prompts[i], results[i]))
}
cat("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ\n")
# æ¸…ç†
backend_free()
gc()
#!/usr/bin/env Rscript
# =============================================================================
# newrllama4 Package Demo - å®Œæ•´åŠŸèƒ½å±•ç¤º
# =============================================================================
cat("ğŸš€ newrllama4 Package Demo\n")
cat("å±•ç¤ºå®Œæ•´çš„LLMæ¨ç†æµç¨‹ï¼šåº“åŠ è½½ â†’ åç«¯å®‰è£… â†’ æ¨¡å‹åŠ è½½ â†’ æ–‡æœ¬ç”Ÿæˆ\n\n")
# 1. åŠ è½½åŒ…
cat("ğŸ“¦ [1/5] åŠ è½½ newrllama4 åŒ…...\n")
library(newrllama4)
# 2. æ£€æŸ¥å¹¶å®‰è£…åç«¯åº“
cat("â¬‡ï¸  [2/5] æ£€æŸ¥é¢„ç¼–è¯‘åç«¯åº“...\n")
if (!lib_is_installed()) {
cat("    æ­£åœ¨ä¸‹è½½é¢„ç¼–è¯‘åº“...\n")
install_newrllama()
} else {
cat("    âœ… åç«¯åº“å·²å®‰è£…\n")
}
# 3. åˆå§‹åŒ–åç«¯
cat("ğŸ”§ [3/5] åˆå§‹åŒ–åç«¯...\n")
backend_init()
# 4. åŠ è½½æ¨¡å‹
cat("ğŸ“š [4/5] åŠ è½½ Llama æ¨¡å‹...\n")
model_path <- "/Users/yaoshengleo/Desktop/ggufæ¨¡å‹/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("âŒ è¯·æ›´æ–°æ¨¡å‹è·¯å¾„\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Metal GPU åŠ é€Ÿ)\n")
# åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# å•åºåˆ—ç”Ÿæˆ
cat("â•â•â• å•åºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("è¾“å…¥: %s\n", prompt))
cat(sprintf("è¾“å‡º: %s\n\n", result))
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s â†’ %s\n", i, prompts[i], results[i]))
}
cat("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ\n")
# æ¸…ç†
backend_free()
library(newrllama4)
model_path <- "/Users/yaoshengleo/Desktop/ggufæ¨¡å‹/gemma-3-12b-it-Q8_0.gguf"
# é¦–å…ˆæ£€æŸ¥å†…å­˜å’ŒåŠ è½½æ¨¡å‹
cat("\n=== å†…å­˜æ£€æŸ¥å’Œæ¨¡å‹åŠ è½½ ===\n")
tryCatch({
# ä¼°ç®—å†…å­˜éœ€æ±‚
estimated_memory <- .Call("c_r_estimate_model_memory", model_path)
cat("ä¼°ç®—å†…å­˜éœ€æ±‚:", round(estimated_memory / (1024^3), 2), "GB\n")
# æ£€æŸ¥å¯ç”¨å†…å­˜
memory_available <- .Call("c_r_check_memory_available", estimated_memory)
cat("å†…å­˜æ˜¯å¦è¶³å¤Ÿ:", memory_available, "\n")
if (!memory_available) {
stop("å†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½12Bæ¨¡å‹")
}
# åŠ è½½æ¨¡å‹
cat("\næ­£åœ¨åŠ è½½12Bæ¨¡å‹...\n")
model <- model_load(model_path,
check_memory = TRUE,
verify_integrity = TRUE,
n_gpu_layers = 0L,
use_mmap = TRUE)
cat("âœ“ 12Bæ¨¡å‹åŠ è½½æˆåŠŸï¼\n")
# åˆ›å»ºä¸Šä¸‹æ–‡
cat("\n=== åˆ›å»ºä¸Šä¸‹æ–‡ ===\n")
context <- context_create(model, n_ctx = 1024, n_threads = 4)
cat("âœ“ ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\n")
# æµ‹è¯•1: å•åºåˆ—ç”Ÿæˆ
cat("\n=== æµ‹è¯•1: å•åºåˆ—æ–‡æœ¬ç”Ÿæˆ ===\n")
test_prompts <- c(
"What is artificial intelligence?",
"Explain quantum computing in simple terms.",
"Write a short poem about nature."
)
# é¦–å…ˆæ£€æŸ¥å†…å­˜å’ŒåŠ è½½æ¨¡å‹
cat("\n=== å†…å­˜æ£€æŸ¥å’Œæ¨¡å‹åŠ è½½ ===\n")
tryCatch({
# ä¼°ç®—å†…å­˜éœ€æ±‚
estimated_memory <- .Call("c_r_estimate_model_memory", model_path)
cat("ä¼°ç®—å†…å­˜éœ€æ±‚:", round(estimated_memory / (1024^3), 2), "GB\n")
# æ£€æŸ¥å¯ç”¨å†…å­˜
memory_available <- .Call("c_r_check_memory_available", estimated_memory)
cat("å†…å­˜æ˜¯å¦è¶³å¤Ÿ:", memory_available, "\n")
if (!memory_available) {
stop("å†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½12Bæ¨¡å‹")
}
# åŠ è½½æ¨¡å‹
cat("\næ­£åœ¨åŠ è½½12Bæ¨¡å‹...\n")
model <- model_load(model_path,
check_memory = TRUE,
verify_integrity = TRUE,
n_gpu_layers = 0L,
use_mmap = TRUE)
cat("âœ“ 12Bæ¨¡å‹åŠ è½½æˆåŠŸï¼\n")
# åˆ›å»ºä¸Šä¸‹æ–‡
cat("\n=== åˆ›å»ºä¸Šä¸‹æ–‡ ===\n")
context <- context_create(model, n_ctx = 1024, n_threads = 4)
cat("âœ“ ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\n")
# æµ‹è¯•1: å•åºåˆ—ç”Ÿæˆ
cat("\n=== æµ‹è¯•1: å•åºåˆ—æ–‡æœ¬ç”Ÿæˆ ===\n")
test_prompts <- c(
"What is artificial intelligence?",
"Explain quantum computing in simple terms.",
"Write a short poem about nature."
)
for (i in seq_along(test_prompts)) {
cat("\n--- æµ‹è¯•", i, "---\n")
cat("è¾“å…¥:", test_prompts[i], "\n")
start_time <- Sys.time()
tryCatch({
tokens <- tokenize(model, test_prompts[i], add_special = TRUE)
result <- generate(context, tokens, max_tokens = 50)
end_time <- Sys.time()
cat("è¾“å‡º:", result, "\n")
cat("ç”Ÿæˆæ—¶é—´:", round(as.numeric(end_time - start_time), 2), "ç§’\n")
cat("âœ“ å•åºåˆ—ç”ŸæˆæˆåŠŸ\n")
}, error = function(e) {
cat("âœ— å•åºåˆ—ç”Ÿæˆå¤±è´¥:", e$message, "\n")
})
}
# æµ‹è¯•2: å¹¶è¡Œç”Ÿæˆï¼ˆå¦‚æœå‡½æ•°å­˜åœ¨ï¼‰
cat("\n=== æµ‹è¯•2: å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆ ===\n")
# æ£€æŸ¥æ˜¯å¦æœ‰å¹¶è¡Œç”Ÿæˆå‡½æ•°
parallel_functions <- c("generate_parallel", "parallel_generate")
parallel_func <- NULL
for (func_name in parallel_functions) {
if (exists(func_name, mode = "function")) {
parallel_func <- get(func_name)
cat("æ‰¾åˆ°å¹¶è¡Œå‡½æ•°:", func_name, "\n")
break
}
}
if (!is.null(parallel_func)) {
# å‡†å¤‡å¹¶è¡Œæµ‹è¯•æ•°æ®
parallel_prompts <- c(
"Tell me about machine learning.",
"What is the future of technology?",
"Describe the solar system."
)
cat("å¼€å§‹å¹¶è¡Œç”Ÿæˆæµ‹è¯•...\n")
start_time <- Sys.time()
tryCatch({
# å°è¯•å¹¶è¡Œç”Ÿæˆ
parallel_results <- parallel_func(
model = model,
prompts = parallel_prompts,
max_tokens = 30,
n_threads = 4
)
end_time <- Sys.time()
parallel_time <- as.numeric(end_time - start_time)
cat("âœ“ å¹¶è¡Œç”ŸæˆæˆåŠŸï¼\n")
cat("å¹¶è¡Œç”Ÿæˆæ—¶é—´:", round(parallel_time, 2), "ç§’\n")
cat("å¹³å‡æ¯ä¸ªåºåˆ—:", round(parallel_time / length(parallel_prompts), 2), "ç§’\n")
# æ˜¾ç¤ºå¹¶è¡Œç»“æœ
for (i in seq_along(parallel_prompts)) {
cat("\n--- å¹¶è¡Œç»“æœ", i, "---\n")
cat("è¾“å…¥:", parallel_prompts[i], "\n")
cat("è¾“å‡º:", parallel_results[i], "\n")
}
}, error = function(e) {
cat("âœ— å¹¶è¡Œç”Ÿæˆå¤±è´¥:", e$message, "\n")
})
} else {
cat("æœªæ‰¾åˆ°å¹¶è¡Œç”Ÿæˆå‡½æ•°ï¼Œè·³è¿‡å¹¶è¡Œæµ‹è¯•\n")
cat("å¯ç”¨çš„ç”Ÿæˆå‡½æ•°:\n")
funcs <- ls(envir = .GlobalEnv)
generate_funcs <- funcs[grepl("generate", funcs, ignore.case = TRUE)]
if (length(generate_funcs) > 0) {
cat(paste(generate_funcs, collapse = ", "), "\n")
} else {
cat("æ— å…¶ä»–ç”Ÿæˆå‡½æ•°\n")
}
}
# æ€§èƒ½æ€»ç»“
cat("\n=== æ€§èƒ½æ€»ç»“ ===\n")
cat("âœ“ 12Bæ¨¡å‹æˆåŠŸåŠ è½½å’Œè¿è¡Œ\n")
cat("âœ“ å•åºåˆ—ç”ŸæˆåŠŸèƒ½æ­£å¸¸\n")
if (!is.null(parallel_func)) {
cat("âœ“ å¹¶è¡Œç”ŸæˆåŠŸèƒ½æµ‹è¯•å®Œæˆ\n")
} else {
cat("- å¹¶è¡Œç”ŸæˆåŠŸèƒ½æœªæ‰¾åˆ°\n")
}
}, error = function(e) {
cat("\nâœ— æµ‹è¯•å¤±è´¥:", e$message, "\n")
# åˆ†æå¤±è´¥åŸå› 
if (grepl("å†…å­˜|memory|Memory", e$message)) {
cat("â†’ å†…å­˜ä¸è¶³å¯¼è‡´çš„å¤±è´¥\n")
cat("â†’ å†…å­˜ä¿æŠ¤æœºåˆ¶æ­£å¸¸å·¥ä½œ\n")
} else if (grepl("model|æ¨¡å‹", e$message)) {
cat("â†’ æ¨¡å‹åŠ è½½ç›¸å…³é”™è¯¯\n")
} else {
cat("â†’ å…¶ä»–é”™è¯¯ç±»å‹\n")
}
})
model <- model_load(model_path,
check_memory = TRUE,
verify_integrity = TRUE,
n_gpu_layers = 0L,
use_mmap = TRUE)
library(newrllama4)
model_path <- "/Users/yaoshengleo/Desktop/ggufæ¨¡å‹/gemma-3-12b-it-Q8_0.gguf"
model1 <- model_load(model_path,
check_memory = FALSE,  # ç¦ç”¨å†…å­˜æ£€æŸ¥
verify_integrity = FALSE,  # ç¦ç”¨å®Œæ•´æ€§æ£€æŸ¥
n_gpu_layers = 0L,
use_mmap = TRUE)
context <- context_create(model1, n_ctx = 512, n_threads = 2)
text <- "Hello"
tokens <- tokenize(model1, text, add_special = TRUE)
cat("âœ“ tokenizeæˆåŠŸï¼Œtokens:", length(tokens), "\n")
result <- generate(context, tokens, max_tokens = 10)
cat("âœ“ æ–‡æœ¬ç”ŸæˆæˆåŠŸ\n")
cat("è¾“å…¥:", text, "\n")
cat("è¾“å‡º:", result, "\n")
# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
cat("\n=== æµ‹è¯•åŸºæœ¬åŠŸèƒ½ ===\n")
context <- context_create(model1, n_ctx = 512, n_threads = 2)
cat("âœ“ ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\n")
text <- "Hello"
tokens <- tokenize(model1, text, add_special = TRUE)
cat("âœ“ tokenizeæˆåŠŸï¼Œtokens:", length(tokens), "\n")
result <- generate(context, tokens, max_tokens = 10)
cat("âœ“ æ–‡æœ¬ç”ŸæˆæˆåŠŸ\n")
cat("è¾“å…¥:", text, "\n")
cat("è¾“å‡º:", result, "\n")
}, error = function(e) {
#!/usr/bin/env Rscript
# =============================================================================
# å¹¶è¡Œç”Ÿæˆå‡½æ•°æ”¹è¿›æµ‹è¯• - v1.0.55
# =============================================================================
cat("ğŸš€ Testing improved parallel generation - v1.0.55\n")
cat("éªŒè¯ç»Ÿä¸€æ‰¹æ¬¡å¤„ç†å’Œä¸¥æ ¼åºåˆ—éš”ç¦»çš„æ–°å®ç°\n\n")
# 1. åŠ è½½åŒ…
cat("ğŸ“¦ [1/4] åŠ è½½ newrllama4 åŒ…...\n")
library(newrllama4)
# 2. æ£€æŸ¥å¹¶å®‰è£…åç«¯åº“
cat("â¬‡ï¸  [2/4] æ£€æŸ¥é¢„ç¼–è¯‘åç«¯åº“...\n")
if (!lib_is_installed()) {
cat("    æ­£åœ¨ä¸‹è½½é¢„ç¼–è¯‘åº“...\n")
install_newrllama()
} else {
cat("    âœ… åç«¯åº“å·²å®‰è£…\n")
}
# 3. åˆå§‹åŒ–åç«¯
cat("ğŸ”§ [3/4] åˆå§‹åŒ–åç«¯...\n")
backend_init()
# 4. åŠ è½½æ¨¡å‹
cat("ğŸ“š [4/4] åŠ è½½ Llama æ¨¡å‹...\n")
model_path <- "/Users/yaoshengleo/Desktop/ggufæ¨¡å‹/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf"
if (!file.exists(model_path)) {
cat("âŒ è¯·æ›´æ–°æ¨¡å‹è·¯å¾„\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Metal GPU åŠ é€Ÿ)\n")
# åˆ›å»ºå¹¶è¡Œæ¨ç†ä¸Šä¸‹æ–‡
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 8L)
# =============================================================================
# æµ‹è¯•3: å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†
# =============================================================================
cat("\nâ•â•â• æµ‹è¯•3: å¤§è§„æ¨¡å¹¶è¡Œå¤„ç† â•â•â•\n")
prompts_large <- c(
"Explain photosynthesis.",
"What is the theory of relativity?",
"Describe the water cycle.",
"What is DNA?",
"Explain gravity.",
"What is the solar system?",
"Describe evolution.",
"What is the periodic table?"
)
cat("è¾“å…¥ 8 ä¸ªç§‘å­¦é—®é¢˜è¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œå¤„ç†...\n")
start_time <- Sys.time()
results_large <- generate_parallel(
context_parallel,
prompts_large,
max_tokens = 20,
temperature = 0.5
)
end_time <- Sys.time()
large_processing_time <- as.numeric(end_time - start_time)
cat("âœ… å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†å®Œæˆ\n")
cat("ğŸ“Š å¤„ç†æ—¶é—´:", round(large_processing_time, 2), "ç§’\n")
cat("ğŸ“Š å¹³å‡æ¯ä¸ªé—®é¢˜:", round(large_processing_time / length(prompts_large), 2), "ç§’\n")
success_count <- 0
for (i in seq_along(prompts_large)) {
result <- results_large[i]
if (!is.null(result) && nchar(result) > 0 && !grepl("\\[ERROR\\]", result)) {
success_count <- success_count + 1
}
}
cat(sprintf("ğŸ“Š æˆåŠŸå¤„ç†ç‡: %d/%d (%.1f%%)\n",
success_count, length(prompts_large),
success_count / length(prompts_large) * 100))
# =============================================================================
# æµ‹è¯•4: é”™è¯¯æ¢å¤èƒ½åŠ›
# =============================================================================
cat("\nâ•â•â• æµ‹è¯•4: é”™è¯¯æ¢å¤èƒ½åŠ› â•â•â•\n")
prompts_mixed <- c(
"What is the capital of France?",
paste(rep("Very long", 100), collapse = " "),  # å¯èƒ½è¶…é•¿çš„æç¤ºç¬¦
"What is 2+2?",
"Tell me a joke."
)
cat("è¾“å…¥åŒ…å«å¯èƒ½é—®é¢˜çš„æ··åˆæç¤ºç¬¦...\n")
results_mixed <- generate_parallel(
context_parallel,
prompts_mixed,
max_tokens = 15L,
temperature = 0.7
)
cat("âœ… é”™è¯¯æ¢å¤æµ‹è¯•å®Œæˆ\n")
for (i in seq_along(prompts_mixed)) {
result <- results_mixed[i]
if (grepl("\\[ERROR\\]", result)) {
cat(sprintf("ğŸ”¹ é—®é¢˜%d: å‡ºç°é”™è¯¯ - %s\n", i, result))
} else {
cat(sprintf("ğŸ”¹ é—®é¢˜%d: æ­£å¸¸å¤„ç† - %s\n", i, substr(result, 1, 50)))
}
}
# æ¸…ç†
cat("\nğŸ§¹ æ¸…ç†èµ„æº...\n")
backend_free()
gc()
cat("ğŸ‰ v1.0.55 æ”¹è¿›ç‰ˆå¹¶è¡Œç”Ÿæˆæµ‹è¯•å®Œæˆï¼\n")
library(newrllama4)
install_newrllama4()
install_newrllama()
response <- quick_llama("Hello, how are you?")
prompts <- c("Summarize machine learning",
"Explain quantum computing",
"What is artificial intelligence?")
responses <- quick_llama(prompts)
print(responses)
