# --- FILE: newrllama4/R/quick_llama.R ---

# Package-level globals for caching
.quick_llama_env <- new.env(parent = emptyenv())

#' Quick LLaMA Inference
#'
#' A high-level convenience function that provides one-line LLM inference.
#' Automatically handles model downloading, loading, and text generation with optional
#' chat template formatting and system prompts for instruction-tuned models.
#'
#' @param prompt Character string or vector of prompts to process
#' @param model Model URL or path (default: Gemma 3 1B IT Q4_0)
#' @param n_threads Number of threads (default: auto-detect)
#' @param n_gpu_layers Number of GPU layers (default: auto-detect)
#' @param n_ctx Context size (default: 2048)
#' @param max_tokens Maximum tokens to generate (default: 100)
#' @param temperature Sampling temperature (default: 0.7)
#' @param top_p Top-p sampling (default: 0.9)
#' @param top_k Top-k sampling (default: 40)
#' @param verbosity Backend logging verbosity (default: 1L). Higher values show more
#'   detail: \code{0} prints only errors, \code{1} adds warnings, \code{2}
#'   includes informational messages, and \code{3} enables the most verbose debug
#'   output.
#' @param repeat_last_n Number of recent tokens to consider for repetition penalty (default: 64)
#' @param penalty_repeat Repetition penalty strength (default: 1.1)
#' @param min_p Minimum probability threshold (default: 0.05)
#' @param system_prompt System prompt to add to conversation (default: "You are a helpful assistant.")
#' @param auto_format Whether to automatically apply chat template formatting (default: TRUE)
#' @param chat_template Custom chat template to use (default: NULL uses model's built-in template)
#' @param stream Whether to stream output (default: auto-detect based on interactive())
#' @param seed Random seed for reproducibility (default: 1234)
#' @param progress Show a console progress bar when running parallel generation
#'   (default: \\code{interactive()}). Has no effect for single-prompt runs.
#' @param ... Additional parameters passed to generate() or generate_parallel()
#'
#' @return Character string (single prompt) or named list (multiple prompts)
#' @export
#' @seealso \code{\link{model_load}}, \code{\link{generate}}, \code{\link{generate_parallel}}, \code{\link{install_newrllama}}
#'
#' @examples
#' \dontrun{
#' # Simple usage with default chat template and system prompt
#' response <- quick_llama("Hello, how are you?")
#' 
#' # Raw text generation without chat template
#' raw_response <- quick_llama("Complete this: The capital of France is", 
#'                            auto_format = FALSE)
#' 
#' # Custom system prompt
#' code_response <- quick_llama("Write a Python hello world program",
#'                             system_prompt = "You are a Python programming expert.")
#' 
#' # No system prompt
#' response <- quick_llama("What is AI?", system_prompt = NULL)
#' 
#' # Multiple prompts with custom settings
#' responses <- quick_llama(c("Summarize AI", "Explain quantum computing"),
#'                         temperature = 0.5, max_tokens = 150)
#' 
#' # High creativity with verbose output
#' creative_response <- quick_llama("Tell me a story", 
#'                                  temperature = 0.9, 
#'                                  max_tokens = 200,
#'                                  verbosity = 0L)
#' 
#' # Custom chat template (if supported by model)
#' custom_response <- quick_llama("Explain photosynthesis",
#'                               chat_template = "<|user|>\n{user}\n<|assistant|>\n")
#' }
quick_llama <- function(prompt,
                        model = .get_default_model(),
                        n_threads = NULL,
                        n_gpu_layers = "auto",
                        n_ctx = 2048L,
                        verbosity = 1L,
                        max_tokens = 100L,
                        top_k = 20L,
                        top_p = 0.9,
                        temperature = 0.7,
                        repeat_last_n = 64L,
                        penalty_repeat = 1.1,
                        min_p = 0.05,
                        system_prompt = "You are a helpful assistant.",
                        auto_format = TRUE,
                        chat_template = NULL,
                        stream = FALSE,
                        seed = 1234L,
                        progress = interactive(),
                        ...) {
  
  # Validate inputs
  if (missing(prompt) || is.null(prompt) || length(prompt) == 0) {
    stop("Prompt cannot be empty", call. = FALSE)
  }
  
  # Check for empty strings
  if (any(nchar(prompt) == 0)) {
    stop("Prompt cannot be empty", call. = FALSE)
  }
  
  # Ensure stream is logical
  stream <- as.logical(stream)
  
  # Auto-detect n_threads if not specified
  if (is.null(n_threads)) {
    n_threads <- max(1L, parallel::detectCores() - 1L)
  }
  
  # Auto-detect n_gpu_layers if specified as "auto"
  if (identical(n_gpu_layers, "auto")) {
    n_gpu_layers <- .detect_gpu_layers()
  }
  
  # Ensure backend is ready
  .ensure_quick_llama_ready()
  
  # Load model and context if not cached or if different model
  tryCatch({
    .ensure_model_loaded(model, n_gpu_layers, n_ctx, n_threads, verbosity)
  }, error = function(e) {
    stop("Failed to load model: ", e$message, call. = FALSE)
  })
  
  # Format prompt with chat template if requested
  if (auto_format) {
    # Create messages structure
    if (!is.null(system_prompt) && nchar(system_prompt) > 0) {
      messages <- list(
        list(role = "system", content = system_prompt),
        list(role = "user", content = prompt)
      )
    } else {
      messages <- list(
        list(role = "user", content = prompt)
      )
    }
    
    # Apply chat template
    formatted_prompt <- apply_chat_template(.quick_llama_env$model, messages, 
                                           template = chat_template, add_assistant = TRUE)
  } else {
    formatted_prompt <- prompt
  }
  
  # Debug: check EOS token (optional)
  if (verbosity <= 1L) {
    eos_token <- tokenize(.quick_llama_env$model, "", add_special = FALSE)
    message("Model EOS token info available for debugging")
  }
  
  # Generate text
  result <- if (length(prompt) == 1) {
    # Single prompt
    .generate_single(formatted_prompt, max_tokens, top_k, top_p, temperature, 
                     repeat_last_n, penalty_repeat, seed, stream)
  } else {
    # Multiple prompts - apply formatting to each prompt
    if (auto_format) {
      formatted_prompts <- sapply(prompt, function(p) {
        if (!is.null(system_prompt) && nchar(system_prompt) > 0) {
          msgs <- list(
            list(role = "system", content = system_prompt),
            list(role = "user", content = p)
          )
        } else {
          msgs <- list(list(role = "user", content = p))
        }
        apply_chat_template(.quick_llama_env$model, msgs, 
                           template = chat_template, add_assistant = TRUE)
      })
    } else {
      formatted_prompts <- prompt
    }
    .generate_multiple(formatted_prompts, max_tokens, top_k, top_p, temperature, 
                       repeat_last_n, penalty_repeat, seed, stream, progress)
  }
  
  # Clean up special tokens from output
  if (auto_format && is.character(result)) {
    if (length(result) == 1) {
      result <- .clean_output(result)
    } else {
      result <- lapply(result, .clean_output)
    }
  }
  
  result
}

#' Reset quick_llama state
#'
#' Clears cached model and context objects, forcing fresh initialization
#' on the next call to quick_llama().
#'
#' @export
quick_llama_reset <- function() {
  if (exists("model", envir = .quick_llama_env)) {
    rm(list = ls(envir = .quick_llama_env), envir = .quick_llama_env)
  }
  message("quick_llama state reset")
  invisible(NULL)
}

# --- Helper Functions ---

#' Clean output by removing special tokens
#' @param text The generated text to clean
#' @return Cleaned text
#' @noRd
.clean_output <- function(text) {
  if (!is.character(text) || length(text) == 0) return(text)
  
  # Remove Llama-3.2 specific chat template tokens (highest priority)
  text <- gsub("<\\|eot_id\\|>.*$", "", text)
  text <- gsub("<\\|start_header_id\\|>.*$", "", text) 
  text <- gsub("<\\|end_header_id\\|>.*$", "", text)
  
  # Remove other common chat template special tokens
  text <- gsub("<\\|im_start\\|>.*$", "", text)
  text <- gsub("<\\|im_end\\|>.*$", "", text)
  text <- gsub("<\\|end\\|>.*$", "", text)
  text <- gsub("<\\|assistant\\|>.*$", "", text)
  text <- gsub("<\\|user\\|>.*$", "", text)
  text <- gsub("<\\|system\\|>.*$", "", text)
  
  # Remove any remaining template tokens (catch-all patterns)
  text <- gsub("<\\|[^|]+\\|>.*$", "", text)
  text <- gsub("<[^>]*>.*$", "", text)
  
  # Trim whitespace
  text <- trimws(text)
  
  return(text)
}

#' Get default model URL
#' @return Default model URL
#' @noRd
.get_default_model <- function() {
  "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf"
}

#' Detect optimal GPU layers
#' @return Integer number of GPU layers
#' @noRd
.detect_gpu_layers <- function() {
  # Try to detect GPU support
  # This is a simplified version - in real implementation, you might
  # want to check for specific GPU libraries or capabilities
  
  sysname <- Sys.info()["sysname"]
  
  # Basic heuristic: if on macOS (likely has Metal), use GPU
  # if on Linux/Windows, be more conservative
  if (sysname == "Darwin") {
    # On macOS, assume Metal is available
    return(999L)  # Use all layers on GPU
  } else if (sysname == "Linux") {
    # On Linux, check if NVIDIA GPU tools are available
    # This is a basic check - more sophisticated detection could be added
    nvidia_smi <- Sys.which("nvidia-smi")
    if (nvidia_smi != "") {
      return(999L)
    }
  }
  
  # Default to CPU-only
  return(0L)
}

#' Ensure backend is ready
#' @noRd
.ensure_quick_llama_ready <- function() {
  # Check if backend library is installed
  if (!lib_is_installed()) {
    message("Backend library not found. Installing...")
    install_newrllama()
  }
  
  # Initialize backend if not already done
  if (!.is_backend_loaded()) {
    message("Initializing backend...")
    backend_init()
  }
}

#' Ensure model and context are loaded
#' @param model_path Model path or URL
#' @param n_gpu_layers Number of GPU layers
#' @param n_ctx Context size
#' @param n_threads Number of threads
#' @param verbosity Verbosity level
#' @noRd
.ensure_model_loaded <- function(model_path, n_gpu_layers, n_ctx, n_threads, verbosity = 1L) {
  # Check if we have a cached model and context for this configuration
  cache_key <- paste0(model_path, "_", n_gpu_layers, "_", n_ctx, "_", n_threads, "_", verbosity)
  
  if (exists("cache_key", envir = .quick_llama_env) && 
      identical(.quick_llama_env$cache_key, cache_key) &&
      exists("model", envir = .quick_llama_env) &&
      exists("context", envir = .quick_llama_env)) {
    # Model and context already loaded with same configuration
    return()
  }
  
  # Load model
  message("Loading model...")
  model_obj <- model_load(model_path, n_gpu_layers = n_gpu_layers, show_progress = TRUE, verbosity = verbosity)
  
  # Create context
  message("Creating context...")
  context_obj <- context_create(model_obj, n_ctx = n_ctx, n_threads = n_threads, verbosity = verbosity)
  
  # Cache the objects
  .quick_llama_env$model <- model_obj
  .quick_llama_env$context <- context_obj
  .quick_llama_env$cache_key <- cache_key
  
  message("Model and context ready!")
}

#' Generate text for single prompt
#' @param prompt Single prompt string
#' @param max_tokens Maximum tokens
#' @param top_k Top-k sampling
#' @param top_p Top-p sampling
#' @param temperature Temperature
#' @param repeat_penalty Repetition penalty
#' @param seed Random seed
#' @param stream Whether to stream
#' @param ... Additional parameters
#' @return Generated text string
#' @noRd
.generate_single <- function(prompt, max_tokens, top_k, top_p, temperature, 
                             repeat_last_n, penalty_repeat, seed, stream, ...) {
  
  context <- .quick_llama_env$context
  model <- .quick_llama_env$model
  
  # Tokenize prompt
  tokens <- tokenize(model, prompt, add_special = TRUE)
  
  # Generate
  # Generate text (streaming flag is available for future use)
  message("Generating...")
  result <- generate(context, tokens, 
                    max_tokens = max_tokens,
                    top_k = top_k,
                    top_p = top_p,
                    temperature = temperature,
                    repeat_last_n = repeat_last_n,
                    penalty_repeat = penalty_repeat,
                    seed = seed)
  result
}

#' Generate text for multiple prompts
#' @param prompts Vector of prompt strings
#' @param max_tokens Maximum tokens
#' @param top_k Top-k sampling
#' @param top_p Top-p sampling
#' @param temperature Temperature
#' @param repeat_last_n Number of recent tokens for repetition penalty
#' @param penalty_repeat Repetition penalty strength
#' @param seed Random seed
#' @param stream Whether to stream
#' @param ... Additional parameters
#' @return Named list of generated texts
#' @noRd
.generate_multiple <- function(prompts, max_tokens, top_k, top_p, temperature, 
                               repeat_last_n, penalty_repeat, seed, stream, progress, ...) {
  
  context <- .quick_llama_env$context
  
  message("Generating ", length(prompts), " responses...")
  
  # Use parallel generation for better performance (streaming flag available for future use)
  results <- generate_parallel(context, prompts,
                              max_tokens = max_tokens,
                              top_k = top_k,
                              top_p = top_p,
                              temperature = temperature,
                              repeat_last_n = repeat_last_n,
                              penalty_repeat = penalty_repeat,
                              seed = seed,
                              progress = progress)
  
  # Return as named list
  names(results) <- paste0("prompt_", seq_along(prompts))
  results
}

#' Check if backend is loaded
#' @return TRUE if backend is loaded, FALSE otherwise
#' @noRd
.is_backend_loaded <- function() {
  # Simply check if the backend library is installed
  # The actual loading will be handled by ensure_backend_loaded
  tryCatch({
    lib_is_installed()
  }, error = function(e) {
    FALSE
  })
}
