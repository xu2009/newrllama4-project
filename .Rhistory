library(newrllama4)
# 2. æ£€æŸ¥å¹¶å®‰è£…åç«¯åº“
cat("â¬‡ï¸  [2/5] æ£€æŸ¥é¢„ç¼–è¯‘åç«¯åº“...\n")
if (!lib_is_installed()) {
cat("    æ­£åœ¨ä¸‹è½½é¢„ç¼–è¯‘åº“...\n")
install_newrllama()
} else {
cat("    âœ… åç«¯åº“å·²å®‰è£…\n")
}
# 3. åˆå§‹åŒ–åç«¯
cat("ğŸ”§ [3/5] åˆå§‹åŒ–åç«¯...\n")
backend_init()
# 4. åŠ è½½æ¨¡å‹
cat("ğŸ“š [4/5] åŠ è½½ Llama æ¨¡å‹...\n")
model_path <- "/Users/yaoshengleo/Desktop/ggufæ¨¡å‹/Llama-3.2-1B-Instruct.Q8_0.gguf"
if (!file.exists(model_path)) {
cat("âŒ è¯·æ›´æ–°æ¨¡å‹è·¯å¾„\n")
quit(status = 1)
}
model <- model_load(model_path, n_gpu_layers = 1000L)
cat("    âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Metal GPU åŠ é€Ÿ)\n")
# åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡
context_single <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 1L)
context_parallel <- context_create(model, n_ctx = 512L, n_threads = 2L, n_seq_max = 4L)
# å•åºåˆ—ç”Ÿæˆ
cat("â•â•â• å•åºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompt <- "Introduce Purdue University."
tokens <- tokenize(model, prompt, add_special = TRUE)
result <- generate(context_single, tokens, max_tokens = 30L, temperature = 0.7)
cat(sprintf("è¾“å…¥: %s\n", prompt))
cat(sprintf("è¾“å‡º: %s\n\n", result))
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
)
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting.",
)
# å¹¶è¡Œåºåˆ—ç”Ÿæˆ
cat("â•â•â• å¹¶è¡Œåºåˆ—æ–‡æœ¬ç”Ÿæˆ â•â•â•\n")
prompts <- c(
"Tell me a joke.",
"The difference between R and Python.",
"What is statistical machine learning?",
"Explain the concept of overfitting."
)
results <- generate_parallel(
context_parallel,
prompts,
max_tokens = 50L,
temperature = 0.7
)
for (i in seq_along(prompts)) {
cat(sprintf("%d. %s â†’ %s\n", i, prompts[i], results[i]))
}
cat("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ\n")
# æ¸…ç†
backend_free()
gc()
