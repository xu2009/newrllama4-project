高层次函数开发方案

核心功能：提供高层函数 quick_llama()，让用户只需写一句 R 代码即可完成模型下载、加载和推理。

⸻

1. 产品目标
	1.	零配置：用户仅需提供 prompt。
	2.	智能分流：
	•	单条 prompt → 单序列生成
	•	多条 prompt → 批量（或并行）生成
	3.	参数友好：如不指定超参即采用官方推荐默认值；如需自定义，直接传同名形参覆盖即可。

⸻

2. 典型用户故事
	•	US-1 初学者输入一句话，立即获得模型回复。
	•	US-2 分析师一次提交多句文本，批量返回结果。
	•	US-3 高级用户需要调整某些参数的值，其余使用默认值。

⸻

3. 默认参数设置

参数	默认值	选择理由（简述）
model	https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf	1 B 参数、Q4_K_M 量化：≈ 0.8 GB；在 8 GB RAM CPU-only 环境可运行；聊天/摘要质量与体积折中最佳。
n_threads	parallel::detectCores() - 1	利用大部分核心但保留 1 核给系统，适配多平台。
n_gpu_layers	自动检测：• 如果存在 CUDA / Metal / Vulkan ⇒ 999 (全部推给 GPU)• 否则 0 (纯 CPU)	免去用户配置；小显存亦可回落 CPU。
n_ctx	2048	支持中等篇幅对话（≈4 K tokens）；再大内存和延迟急剧上涨。
max_tokens	100	单次生成不易跑飞，满足回答/摘要常规长度。
temperature	0.7	兼顾创造性与稳定性；0.7 属常用中值。
top_p	0.9	Nucleus 采样默认经验值，保留 90 % 累积概率。
top_k	40	Top-k 截断—速度快且避免稀奇词。
repeat_penalty	1.1	轻度抑制机械复读，保证流畅度。
min_p	0.05	与 top-p 搭配，防止陷入极小概率循环。
stream	interactive()	交互终端默认逐 token 打印；脚本模式整段返回。
seed    1234  随机种子，确保结果可复现。

用户若需修改，只需在调用中直接写同名参数，例如：
quick_llama("Hi", temperature = 0.3, max_tokens = 256)

⸻
